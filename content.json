{"meta":{"title":"Peachey Blog","subtitle":"Never Give Up","description":null,"author":"Peachey","url":"peachey.blog"},"pages":[{"title":"自我介绍","date":"2018-05-05T12:25:05.000Z","updated":"2018-05-05T12:30:43.000Z","comments":true,"path":"about/index.html","permalink":"peachey.blog/about/index.html","excerpt":"","text":""}],"posts":[{"title":"dubbo通信源码解析","slug":"java-dubbo-exchange","date":"2018-08-27T12:03:44.000Z","updated":"2018-09-03T09:20:50.284Z","comments":true,"path":"2018/08/27/java-dubbo-exchange/","link":"","permalink":"peachey.blog/2018/08/27/java-dubbo-exchange/","excerpt":"","text":"1. 网络协议模型 此部分为前提，可能大家都清楚，但是为了理解方便，此小节为基础知识，作为分享dubbo协议与hessian序列化的先导。 OSI与TCP/IP TCP/IP模型 应用层 传输层 网络层 链路层 OSI模型 应用层 展示层 会话层 传输层 网络层 数据链路层 物理层 如上是网络协议通讯模型，其中OSI模型为标准模型，TCP/IP却是实际使用的模型，其中OSI的应用层、展示层和会话层对应TCP/IP中的应用层，OSI的数据链路层和物理层对应TCP/IP的链路层，从请求方到到接收方大概是一个如下的过程： 1234567891011 请求方 接收方应用层 --&gt; data data --&gt; 应用层 | | ∧ ∧ ∨ ∨ | |传输层 --&gt; segment segment --&gt; 传输层 | | ∧ ∧ ∨ ∨ | |网络层 --&gt; package package --&gt; 网络层 | | ∧ ∧ ∨ ∨ | |链路层 --&gt; frame &gt;&gt;传输&gt;&gt; frame --&gt; 链路层 dubbo协议是基于socket进行通讯的，其通讯过程在各层对应含义如下： 应用层 data = hessian序列化后的dubbo协议头 + hessian序列化后的数据体 传输层 segment = tcp协议头 + data 网络层 package = ip协议头 + segment 链路层 frame = frame头 + package + frame尾 从这里可以领会到为什么hessian是可以跨语言的，因为在通信过程中，消费端和服务端之间传输的是二进制数据，也就是说，只要和对应的暴露服务的端口建立tcp链接，发送hessian序列化后的符合dubbo规则的数据，可以实现dubbo接口的调用，但是跨语言的前提需要利用当前语言实现一套hessian的序列化协议。 2. dubbo协议123456789-----------------------------------------------------------------------------------------------| Bit offset | 0-7 | 8-15 | 16-20 | 21 | 22 | 23 | 24-31 |-----------------------------------------------------------------------------------------------| 0 | Magic High | Magic Low | Serialization id | event | Two way | Req/res | status |-----------------------------------------------------------------------------------------------| 32-95 | request id (long) |-----------------------------------------------------------------------------------------------| 96-127 | data length |----------------------------------------------------------------------------------------------- 以下来分别解释各个字段的含义： Magic High: dubbo协议的魔数高8位，固定为0xda,何为魔数，类似java类字节码的魔数，是一个标识符，说明这是dubbo协议。 Magic Low: dubbo协议的魔数低8位，固定为0xbb Serialization id: 序列化类型，占用4位，dubbo支持类型：2 hessian3 java4 compacted java6 fastjson7 native java8 kryo9 fst event: 表示该消息是否为事件，比如心跳事件，为1时候表示事件，占用1位 Two way: 只在请求时有用，表示是否需要服务器返回response，为1时表示需要，占用1位 Req/res:标识是请求还是响应，request: 1, response: 0，占用1位 status: 仅对于response有意义，用于表示response的状态，占用1字节，例举如下：20 OK30 客户端超时31 服务端超时40 错误的请求50 错误的响应60 找不到服务70 服务出错80 服务器出错90 客户端错误100 线程池打满 request id: 标识一个唯一的请求, 占用8个字节，响应此请求时也要利用相同的id data length:这个字段占用4个字节，意思很明显是数据体的长度，单位是字节 native java: 原生java序列化实现compacted Java：压缩java序列化，主要是在原生java序列化基础上，实现了自定义的类描述符写入和读取，写Object类型的类描述符只写入类名称，而不是类的完整信息。这样有很多Object类型的情况下可以减少序列化后的size。java: 只是对原生java序列化和压缩java序列化的封装。kryo,fst只是专门针对java语言的，相对与要比hessian性能高点（未实际测试过），但是hessian支持跨语言 4.1 编码流程4.1.1 编码请求（request）编码dubbo协议头1234567891011121314151617/** * com.alibaba.dubbo.remoting.exchange.codec.ExchangeCodec#encodeRequest */// 获取序列化器方式，默认获取的是com.alibaba.dubbo.common.serialize.hessian2.Hessian2SerializationSerialization serialization = getSerialization(channel);// 创建dubbo协议头，16字节byte[] header = new byte[HEADER_LENGTH];// 设置魔数，两个字节Bytes.short2bytes(MAGIC, header);// FLAG_REQUEST=0x80 相当于将字节第2位 置为1, serialization.getContentTypeId()即为序列化类型，hessian固定为2header[2] = (byte) (FLAG_REQUEST | serialization.getContentTypeId());// FLAG_TWOWAY=0x40 相当于将字节第2位 置为1if (req.isTwoWay()) header[2] |= FLAG_TWOWAY;// FLAG_EVENT=0x20 相当于将字节第3位 置为1if (req.isEvent()) header[2] |= FLAG_EVENT;// 设置请求的id 唯一Bytes.long2bytes(req.getId(), header, 4); 更新游标12int savedWriteIndex = buffer.writerIndex();buffer.writerIndex(savedWriteIndex + HEADER_LENGTH); 编码数据体编码数据体分为事件数据或请求数据，事件数据直接编码，请求的话还有一些特殊处理。 编码请求 12345678910111213141516171819202122/* * com.alibaba.dubbo.rpc.protocol.dubbo.DubboCodec#encodeRequestData */RpcInvocation inv = (RpcInvocation) data;//编码dubbo版本，默认是2.0.0out.writeUTF(inv.getAttachment(Constants.DUBBO_VERSION_KEY, DUBBO_VERSION));//编码全类名out.writeUTF(inv.getAttachment(Constants.PATH_KEY));//编码接口版本out.writeUTF(inv.getAttachment(Constants.VERSION_KEY));//编码方法名称out.writeUTF(inv.getMethodName());//编码形参类型out.writeUTF(ReflectUtils.getDesc(inv.getParameterTypes()));//编码实参Object[] args = inv.getArguments();if (args != null) for (int i = 0; i &lt; args.length; i++) &#123; out.writeObject(encodeInvocationArgument(channel, inv, i)); &#125;//编码隐式参数out.writeObject(inv.getAttachments()); writeUTF实际上是对字符串类型的序列化,并进行UTF-8编码, writeObject实际是序列化的一个入口; 可以通过 RpcContext 上的 setAttachment 和 getAttachment 在服务消费方和提供方之间进行参数的隐式传递。完成一次setAttachment/getAttachment远程调用会被清空，即多次远程调用要多次设置,注意path, group, version, dubbo, token, timeout 几个 key 是保留字段 获取类型描述 123456789101112131415161718192021/* * com.alibaba.dubbo.common.utils.ReflectUtils#getDesc(java.lang.Class&lt;?&gt;) */if (c.isPrimitive()) &#123; // 获取原始类型描述 String t = c.getName(); if (\"void\".equals(t)) ret.append(JVM_VOID); else if (\"boolean\".equals(t)) ret.append(JVM_BOOLEAN); else if (\"byte\".equals(t)) ret.append(JVM_BYTE); else if (\"char\".equals(t)) ret.append(JVM_CHAR); else if (\"double\".equals(t)) ret.append(JVM_DOUBLE); else if (\"float\".equals(t)) ret.append(JVM_FLOAT); else if (\"int\".equals(t)) ret.append(JVM_INT); else if (\"long\".equals(t)) ret.append(JVM_LONG); else if (\"short\".equals(t)) ret.append(JVM_SHORT);&#125; else &#123; // 获取非原始类型描述 ret.append(`L`); ret.append(c.getName().replace(`.`, `/`)); ret.append(`;`);&#125; 获取类型描述的时候，分为原始类型和对象 原始类型：void:V, boolean:Z, byte:B, char:C, double:D, float:F, int:I,long:J,short:S; 对象类型：会将全类名转换得到类型描述，比如java.lang.String:Ljava/lang/String; writeObject 12345/* * com.alibaba.com.caucho.hessian.io.Hessian2Output#writeObject */serializer = findSerializerFactory().getSerializer(object.getClass());serializer.writeObject(object, this); 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768/* * com.alibaba.com.caucho.hessian.io.Hessian2Output */addBasic(void.class, \"void\", BasicSerializer.NULL);addBasic(Boolean.class, \"boolean\", BasicSerializer.BOOLEAN);addBasic(Byte.class, \"byte\", BasicSerializer.BYTE);addBasic(Short.class, \"short\", BasicSerializer.SHORT);addBasic(Integer.class, \"int\", BasicSerializer.INTEGER);addBasic(Long.class, \"long\", BasicSerializer.LONG);addBasic(Float.class, \"float\", BasicSerializer.FLOAT);addBasic(Double.class, \"double\", BasicSerializer.DOUBLE);addBasic(Character.class, \"char\", BasicSerializer.CHARACTER_OBJECT);addBasic(String.class, \"string\", BasicSerializer.STRING);addBasic(Object.class, \"object\", BasicSerializer.OBJECT);addBasic(java.util.Date.class, \"date\", BasicSerializer.DATE);addBasic(boolean.class, \"boolean\", BasicSerializer.BOOLEAN);addBasic(byte.class, \"byte\", BasicSerializer.BYTE);addBasic(short.class, \"short\", BasicSerializer.SHORT);addBasic(int.class, \"int\", BasicSerializer.INTEGER);addBasic(long.class, \"long\", BasicSerializer.LONG);addBasic(float.class, \"float\", BasicSerializer.FLOAT);addBasic(double.class, \"double\", BasicSerializer.DOUBLE);addBasic(char.class, \"char\", BasicSerializer.CHARACTER);addBasic(boolean[].class, \"[boolean\", BasicSerializer.BOOLEAN_ARRAY);addBasic(byte[].class, \"[byte\", BasicSerializer.BYTE_ARRAY);addBasic(short[].class, \"[short\", BasicSerializer.SHORT_ARRAY);addBasic(int[].class, \"[int\", BasicSerializer.INTEGER_ARRAY);addBasic(long[].class, \"[long\", BasicSerializer.LONG_ARRAY);addBasic(float[].class, \"[float\", BasicSerializer.FLOAT_ARRAY);addBasic(double[].class, \"[double\", BasicSerializer.DOUBLE_ARRAY);addBasic(char[].class, \"[char\", BasicSerializer.CHARACTER_ARRAY);addBasic(String[].class, \"[string\", BasicSerializer.STRING_ARRAY);addBasic(Object[].class, \"[object\", BasicSerializer.OBJECT_ARRAY);_staticSerializerMap.put(Class.class, new ClassSerializer());_staticDeserializerMap.put(Number.class, new BasicDeserializer(BasicSerializer.NUMBER));_staticSerializerMap.put(BigDecimal.class, new StringValueSerializer());_staticDeserializerMap.put(BigDecimal.class, new StringValueDeserializer(BigDecimal.class));_staticDeserializerMap.put(BigInteger.class, new BigIntegerDeserializer());_staticSerializerMap.put(File.class, new StringValueSerializer());_staticDeserializerMap.put(File.class, new StringValueDeserializer(File.class));_staticSerializerMap.put(ObjectName.class, new StringValueSerializer());_staticDeserializerMap.put(ObjectName.class, new StringValueDeserializer(ObjectName.class));_staticSerializerMap.put(java.sql.Date.class, new SqlDateSerializer());_staticSerializerMap.put(java.sql.Time.class, new SqlDateSerializer());_staticSerializerMap.put(java.sql.Timestamp.class, new SqlDateSerializer());_staticSerializerMap.put(java.io.InputStream.class, new InputStreamSerializer());_staticDeserializerMap.put(java.io.InputStream.class, new InputStreamDeserializer());_staticDeserializerMap.put(java.sql.Date.class, new SqlDateDeserializer(java.sql.Date.class));_staticDeserializerMap.put(java.sql.Time.class, new SqlDateDeserializer(java.sql.Time.class));_staticDeserializerMap.put(java.sql.Timestamp.class, new SqlDateDeserializer(java.sql.Timestamp.class));_staticDeserializerMap.put(StackTraceElement.class, new StackTraceElementDeserializer());//Java 8_staticSerializerMap.put(Class.forName(\"java.time.LocalTime\"), create(LocalTimeHandle.class));_staticSerializerMap.put(Class.forName(\"java.time.LocalDate\"), create(LocalDateHandle.class));_staticSerializerMap.put(Class.forName(\"java.time.LocalDateTime\"), create(LocalDateTimeHandle.class));_staticSerializerMap.put(Class.forName(\"java.time.Instant\"), create(InstantHandle.class));_staticSerializerMap.put(Class.forName(\"java.time.Duration\"), create(DurationHandle.class));_staticSerializerMap.put(Class.forName(\"java.time.Period\"), create(PeriodHandle.class));_staticSerializerMap.put(Class.forName(\"java.time.Year\"), create(YearHandle.class));_staticSerializerMap.put(Class.forName(\"java.time.YearMonth\"), create(YearMonthHandle.class));_staticSerializerMap.put(Class.forName(\"java.time.MonthDay\"), create(MonthDayHandle.class));_staticSerializerMap.put(Class.forName(\"java.time.OffsetDateTime\"), create(OffsetDateTimeHandle.class));_staticSerializerMap.put(Class.forName(\"java.time.ZoneOffset\"), create(ZoneOffsetHandle.class));_staticSerializerMap.put(Class.forName(\"java.time.OffsetTime\"), create(OffsetTimeHandle.class));_staticSerializerMap.put(Class.forName(\"java.time.ZonedDateTime\"), create(ZonedDateTimeHandle.class)); 在执行writeObject时，会寻找对应的序列化器，有三个Map来存储序列化器, 具体如何序列化某种类型参见3.3 序列化详述： _staticSerializerMap: 存放序列化器，key是class, value是对象 _staticDeserializerMap: 存放反序列化器，key是class, value是对象 _staticTypeMap: 存放反序列化器，key是类型字符串，value是对象 检查size12345678910111213/* * com.alibaba.dubbo.remoting.transport.AbstractCodec#checkPayload */int payload = Constants.DEFAULT_PAYLOAD;if (channel != null &amp;&amp; channel.getUrl() != null) &#123; //会从xml配置文件中取值payload，没有配置默认8M payload = channel.getUrl().getParameter(Constants.PAYLOAD_KEY, Constants.DEFAULT_PAYLOAD);&#125;if (payload &gt; 0 &amp;&amp; size &gt; payload) &#123; ExceedPayloadLimitException e = new ExceedPayloadLimitException(\"Data length too large: \" + size + \", max payload: \" + payload + \", channel: \" + channel); logger.error(e); throw e;&#125; 8M的异常就在这里抛出来的,可以通过为接口配置payload字段来改变传输size, 可以全局配置，也可以指定接口配置，但是增大的同时，会有如下问题： 序列化大量数据性能很差 服务调用可能会超时，导致调用失败 传输大量数据会占用带宽，可能会影响其他服务调用 写数据，更新游标1234// 一个很奇怪的地方，写数据时候是先把header的地方空出来，先写body，最后再写header，没想明白buffer.writerIndex(savedWriteIndex);buffer.writeBytes(header); // write header.buffer.writerIndex(savedWriteIndex + HEADER_LENGTH + len); 4.1.2 编码响应（response）","categories":[{"name":"Java","slug":"Java","permalink":"peachey.blog/categories/Java/"}],"tags":[{"name":"Dubbo","slug":"Dubbo","permalink":"peachey.blog/tags/Dubbo/"}]},{"title":"单例模式完全解析","slug":"java-singleton-pattern","date":"2018-07-21T07:39:16.000Z","updated":"2018-07-23T15:59:43.731Z","comments":true,"path":"2018/07/21/java-singleton-pattern/","link":"","permalink":"peachey.blog/2018/07/21/java-singleton-pattern/","excerpt":"","text":"在天天的劳民伤财的搬砖工作中，每天不是和产品讨论需求，然后就是在苦逼的看着交互，琢磨如何来实现这个需求，某天我翻起tomcat源码，看着main函数中的那个synchronized锁，我懵了，为啥这里要加锁呢？怎么看的这代码似曾相识，这不是失传江湖已久的单例模式么！但标准的不都是双重判断么？这里少了一次哎~真是工作时间长了，连我爸妈都不记得长什么样了！ 单例模式的前世今生单例模式是设计模式中算是最简单、最易懂的设计模式之一了，简单来说，单例模式就是无论什么时候来调用、无论谁来调用，返回的始终是同一个实例，没有其他，于是这就产生了一系列的问题。比如如何保证单例的唯一性，有人会说加锁啊！是的加锁可以在一定程度上解决这个问题，那在高并发的场景下加锁是不是自杀呢？有人说用双重判断咯，不对整个函数加锁啊！那我用序列化和反序列化来替换你的实例，那算不算破坏单例模式呢？以下老司机带你来感受以下单例模式的各种写法 无锁、线程不安全的、最初级写法的懒汉式12345678910public class Singleton &#123; private static Singleton instance; private Singleton ()&#123;&#125; public static Singleton getInstance() &#123; if (instance == null) &#123; instance = new Singleton(); &#125; return instance; &#125;&#125; 此种写法只能保证单线程下为单例模式，如果是web请求，一次请求起一个线程的话，那手动连续点两次就可能创建两个实例，所以基本上也没有人这么写。虽然如此，但是这种写法的也体现了单例模式的几点特性： 用静态变量来声明单例：可以保证全局就一个引用 私有的构造方法：防止外部new新对象 判断instace为空：防止获取实例每次都创建新的对象 同步函数，线程安全，懒汉式12345678910public class Singleton&#123; private static Singleton instance; private Singleton()&#123;&#125; public synchronized static Singleton getInstance()&#123; if (instance == null) &#123; instance = new Singleton(); &#125; return instance; &#125;&#125; 此种写法简单粗暴的对整个函数加锁，这种方式虽然达到了并发获取实例时保证单例的目的，但是也导致了一个问题，每次请求都会执行到synchronized锁，相较而言性能上有损失。 双检锁，线程安全，懒汉式1234567891011121314public class Singleton&#123; private volatile static Singleton instance; private Singleton()&#123;&#125; public static Singleton getInstance()&#123; if (instance == null) &#123; synchronized(this)&#123; if(instance == null)&#123; instance = new Singleton(); &#125; &#125; &#125; return instance; &#125;&#125; 此种写法算是一种比较合理的写法，将同步函数换成同步代码块，并且进行两次判空，保证每次执行都不一定执行到同步锁，从而可以很有效的提升性能。同时与以上不同的是，实例的声明多加了个字段volatile,原因在于instance=new Singleton()是一个非原子的操作，这条语句完成了如下几件事情： 为对象分配内存空间 生成Singleton对象 将instance引用指向内存空间 由于Jvm存在指令重排序情况，这样会导致以上的几个步骤并非是按照顺序依次执行，比如：当线程1刚好将instance指向内存空间，但是还未进行对象的初始化，这个时候线程2正好进行第一重判断，发现不为空，然后直接返回instance，这时候线程2取到的实例必然是有问题的。因此volatile的一个特性便是禁止指令重排序，从而避免上述问题的发生，其实这个应该说是在jdk1.5以后才完全实现的，因为在volatile变量前后的语句也存在指令重排序的问题，具体细节待探究。 简单粗暴，饿汉式1234567public class Singleton&#123; private static final instance = new Singleton(); private Singleton()&#123;&#125; private static Singleton getInstance()&#123; return instance; &#125;&#125; 此种写法是利用了类加载器的机制避免了多线程的同步问题，简单粗暴，但是也存在缺点，便是非懒加载，当初始化对象很复杂时候需要消耗系统很多资源。 静态内部类方式123456789public class Singleton&#123; private static final SingletonHolder&#123; private static final Singleton instance = new Singleton(); &#125; private Singleton()&#123;&#125; public static final Singleton getInstance()&#123; return SingletonHolder.instance; &#125;&#125; 此种写法利用一个静态内部类来持有实例，同样是利用了类加载器机制来避免了多线程同步的问题，相较饿汉式，此种写法支持懒加载，原因在于SingleHolder并未被显示被装载，只有首次调用时候才会显式的装载SingletonHolder并创建Singleton对象，从而达到懒加载的效果，具体细节待探究。 枚举方式123public enum Singleton&#123; Instance;&#125; 此种写法利用枚举的特性，可以完全的达到单例，在不使用发射的前提下，而且此种写法简单易懂，但是目前的这种使用方式不多。 浅谈关于单例的序列化在很多场景下，返回的实例都是实现了Serializable, 这就存在让人利用序列化/反序列化的方式来替换实例，以达到攻击或者其他目的。既然我们在代码中采用单例模式，初衷便是不希望有使用者能够通过某种方式来进行攻击和破坏，为了应对这种问题，可以在对象中实现readResolve方法，此方法的作用便是在反序列化的到一个对象时，会判断该对象有没有实现该方法，如果存在该方法的话，便会调用该方法返回一个对象，强制将反序列化后得到的对象引用指向readResolve的对象，从而使反序列化得到的对象成为垃圾对象，被收集器回收。但是此种方法必须用于辅助的一个手段，便是将包含实例对象类型的变量全部声明为transient,否则还是可以收到一种很复杂的攻击，即在readResolve方法运行之前，攻击者将实例的一个引用放到静态域来保护该非法引用，从而使得在反序列化后依然能够生成一个新的实例，从而破坏单例模式，上述所说的枚举方式是可以完全避免此种攻击方式（详细可以参见【effective java的76条】）。 即使完全的能够屏蔽反序列化的攻击方式，但是这都是建立在不使用反射的前提之下，也就是说不可能绝对的安全。因为完全可以利用反射将实例的访问权限改变，从而粗暴的破坏单例模式。对于单例模式的使用而言，是根据不同的场景和不同的需求来进行选择不同的实现方式，没有绝对安全的编码，同时这也是仁者见仁智者见智。","categories":[{"name":"Java","slug":"Java","permalink":"peachey.blog/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"peachey.blog/tags/Java/"},{"name":"设计模式","slug":"设计模式","permalink":"peachey.blog/tags/设计模式/"}]},{"title":"基于Mybatis的注解式分页插件","slug":"java-mybatis-paging-interceptor","date":"2018-07-05T08:52:06.000Z","updated":"2018-07-13T07:18:36.211Z","comments":true,"path":"2018/07/05/java-mybatis-paging-interceptor/","link":"","permalink":"peachey.blog/2018/07/05/java-mybatis-paging-interceptor/","excerpt":"","text":"","categories":[{"name":"Java","slug":"Java","permalink":"peachey.blog/categories/Java/"}],"tags":[{"name":"Mybatis","slug":"Mybatis","permalink":"peachey.blog/tags/Mybatis/"}]},{"title":"两个排序数组的中位数","slug":"al-median","date":"2018-05-16T14:23:28.000Z","updated":"2018-05-16T14:32:54.000Z","comments":true,"path":"2018/05/16/al-median/","link":"","permalink":"peachey.blog/2018/05/16/al-median/","excerpt":"","text":"题目给定两个大小为 m 和 n 的有序数组 nums1 和 nums2 。请找出这两个有序数组的中位数。要求算法的时间复杂度为 O(log (m+n)) 。 示例 1:1234nums1 = [1, 3]nums2 = [2]中位数是 2.0 示例 2:1234nums1 = [1, 2]nums2 = [3, 4]中位数是 (2 + 3)/2 = 2.5 思路 两个数组长度之和为奇数，则取中间值，为偶数，取中间二值除以2 游标p1,p2,对比p1与p2指向的数，哪个小，哪个游标+1 直到遍历层数到两个数组长度中间值截止 代码 40ms 战胜99.64%的java提交记录12345678910111213141516171819202122232425262728293031class Solution &#123; public double findMedianSortedArrays(int[] nums1, int[] nums2) &#123; int len1 = nums1.length, len2 = nums2.length; int mid = (len1 + len2)/2; int p1 = 0, p2=0 , result = 0, current = 0; boolean single = (len1 + len2)%2 == 1; for(int i=0; i&lt;= mid; i++)&#123; if(p1&gt;= len1)&#123; current = nums2[p2]; p2++; &#125; else if(p2&gt;=len2)&#123; current = nums1[p1]; p1++; &#125; else if(nums1[p1] &gt; nums2[p2])&#123; current = nums2[p2]; p2++; &#125; else&#123; current = nums1[p1]; p1++; &#125; if(i == (mid-1))&#123; result += current; &#125; if(i == mid)&#123; return single ? current : (result + current)/2.0; &#125; &#125; return 0; &#125;&#125;","categories":[{"name":"算法","slug":"算法","permalink":"peachey.blog/categories/算法/"}],"tags":[{"name":"LeetCode","slug":"LeetCode","permalink":"peachey.blog/tags/LeetCode/"}]},{"title":"无重复字符的最长子串","slug":"al-no-repeat-str","date":"2018-05-16T10:47:17.000Z","updated":"2018-05-16T11:06:23.000Z","comments":true,"path":"2018/05/16/al-no-repeat-str/","link":"","permalink":"peachey.blog/2018/05/16/al-no-repeat-str/","excerpt":"","text":"题目给定一个字符串，找出不含有重复字符的最长子串的长度。 示例：给定 “abcabcbb” ，没有重复字符的最长子串是 “abc” ，那么长度就是3。给定 “bbbbb” ，最长的子串就是 “b” ，长度是1。给定 “pwwkew” ，最长子串是 “wke” ，长度是3。请注意答案必须是一个子串，”pwke” 是 子序列 而不是子串。 思路 用两个指针，一个指定开始位置，另外一个作为游标 利用一个数组来确定游标指向的字符，是否之前出现过，数组元素的初始值为-1，数组的下标表示字符的ascii码,数组下标的值为字符在字符串中的位置 遍历字符串时，判断该字符在数组中对应的值若不为-1，则之前出现过，然后计算游标与开始间的距离，若大于max，则作为最大值，重新初始化数组，如此循环 代码 34 ms 战胜94.34%java提交记录1234567891011121314151617181920212223242526272829class Solution &#123; public int lengthOfLongestSubstring(String s) &#123; int start = 0, point = 0; int max = 0; int[] position = new int[128]; for (int i = 0; i &lt; position.length; i++) &#123; position[i] = -1; &#125; while (point &lt; s.length()) &#123; if (position[s.charAt(point)] == -1) &#123; position[s.charAt(point)] = point; point++; &#125; else &#123; if ((point - start) &gt; max) &#123; max = point == s.length() - 1 &amp;&amp; position[s.charAt(point)] == -1 ? point - start + 1 : point - start; &#125; start = position[s.charAt(point)] + 1; point = start; for(int i = 0; i&lt;128; i++)&#123; position[i] = -1; &#125; &#125; &#125; if ((point - start) &gt; max) &#123; max = point == s.length() - 1 &amp;&amp; position[s.charAt(point)] == -1 ? point - start + 1 : point - start; &#125; return max; &#125;&#125;","categories":[{"name":"算法","slug":"算法","permalink":"peachey.blog/categories/算法/"}],"tags":[{"name":"LeetCode","slug":"LeetCode","permalink":"peachey.blog/tags/LeetCode/"}]},{"title":"两数相加","slug":"al-two-plus","date":"2018-05-11T04:35:28.000Z","updated":"2018-05-16T11:06:33.000Z","comments":true,"path":"2018/05/11/al-two-plus/","link":"","permalink":"peachey.blog/2018/05/11/al-two-plus/","excerpt":"","text":"题目给定两个非空链表来表示两个非负整数。位数按照逆序方式存储，它们的每个节点只存储单个数字。将两数相加返回一个新的链表。你可以假设除了数字 0 之外，这两个数字都不会以零开头。 示例：输入：(2 -&gt; 4 -&gt; 3) + (5 -&gt; 6 -&gt; 4)输出：7 -&gt; 0 -&gt; 8原因：342 + 465 = 807 我的解法 34ms 战胜96.11%的java 提交记录核心思路 两个数与进位相加 %10余数为当前节点值 /10除数为进位值 循环条件：两个链表一个不为空，或carry大于0遍 代码123456789101112131415161718192021222324252627282930313233343536373839/** * Definition for singly-linked list. * public class ListNode &#123; * int val; * ListNode next; * ListNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; public ListNode addTwoNumbers(ListNode l1, ListNode l2) &#123; int num1 = l1.val; int num2 = l2.val; int result = num1 + num2; int carry = result / 10; ListNode head = new ListNode(result % 10); ListNode point = head; l1 = l1.next; l2 = l2.next; while (l1 != null || l2 != null || carry &gt; 0) &#123; num1 = num2 = 0; if (l1 != null) &#123; num1 = l1.val; l1 = l1.next; &#125; if (l2 != null) &#123; num2 = l2.val; l2 = l2.next; &#125; result = num1 + num2 + carry; point.next = new ListNode(result % 10); carry = result / 10; point = point.next; &#125; return head; &#125;&#125;","categories":[{"name":"算法","slug":"算法","permalink":"peachey.blog/categories/算法/"}],"tags":[{"name":"LeetCode","slug":"LeetCode","permalink":"peachey.blog/tags/LeetCode/"}]},{"title":"两数之和","slug":"al-two-sum","date":"2018-05-06T14:24:29.000Z","updated":"2018-05-16T11:06:42.000Z","comments":true,"path":"2018/05/06/al-two-sum/","link":"","permalink":"peachey.blog/2018/05/06/al-two-sum/","excerpt":"","text":"题目给定一个整数数组和一个目标值，找出数组中和为目标值的两个数。你可以假设每个输入只对应一种答案，且同样的元素不能被重复利用。 示例:给定 nums = [2, 7, 11, 15], target = 9因为 nums[0] + nums[1] = 2 + 7 = 9所以返回 [0, 1] 我的解法 33ms 战胜56.94%Java提交123456789101112class Solution &#123; public int[] twoSum(int[] nums, int target) &#123; for(int i=0; i&lt;nums.length-1; i++)&#123; for(int j= i+1; j&lt; nums.length;j++)&#123; if(nums[i]+nums[j] == target)&#123; return new int[]&#123;i, j&#125;; &#125; &#125; &#125; return null; &#125;&#125; 最优解法 3ms12345678910111213141516171819202122232425262728293031323334353637383940414243444546class Solution &#123; public static int[] twoSum(int[] nums, int target) &#123; int max = Integer.MIN_VALUE; int min = Integer.MAX_VALUE; //记录nums中的最大值和最小值 for (int i = 0; i &lt; nums.length; i++) &#123; if (nums[i] &gt; max) max = nums[i]; if (nums[i] &lt; min) min = nums[i]; &#125; //目标值相对最大值和最小值的距离 int maxS = target - min; int minS = target - max; //可以确定其中一个加数最小取值只能是nums最小值，另一个加数的最大值只能是target-min, //而该加数必然在min~max直接，因此maxS最大只能取到max maxS = (maxS &lt; max ? maxS : max); //可以确定其中一个加数的最大值只能是nums的最大值，则另一个加数的最小值只能是target-max, //而该加数min~max直接，因此minS最小只能取到min minS = (minS &gt; min ? minS : min); //从而可以确定每个加数必然在minS~maxS之间 int a[] = new int[maxS - minS + 1]; for (int i = 0; i &lt; a.length; i++) a[i] = -1; //minS代表目标值相对base的距离，a的索引下标代表某值相对target的距离， //a的索引值为nums的索引下标，对于nums[i]来说，寻找一个与之和为target， //需要找一个值为target-num[i]的数，也就是查看a以该数相对target的距离值为下标的位置是否存在值， //存在的话，该位置存储的nums索引下标与i即为结果; //选择target相对max的距离为了保证以上的值为正数，也即是保证a的下标值为正数 for (int i = 0; i &lt; nums.length; i++) &#123; if (nums[i] &gt;= minS &amp;&amp; nums[i] &lt;= maxS) &#123; if (a[target - nums[i] - minS] != -1) return new int[] &#123; a[target - nums[i] - minS], i &#125;; else a[nums[i] - minS] = i; &#125; &#125; return null; &#125;&#125;","categories":[{"name":"算法","slug":"算法","permalink":"peachey.blog/categories/算法/"}],"tags":[{"name":"LeetCode","slug":"LeetCode","permalink":"peachey.blog/tags/LeetCode/"}]},{"title":"技术之路","slug":"study-plan","date":"2018-05-04T15:14:41.000Z","updated":"2018-09-09T08:29:33.646Z","comments":true,"path":"2018/05/04/study-plan/","link":"","permalink":"peachey.blog/2018/05/04/study-plan/","excerpt":"","text":"根据目前的工作而言，个人总结了一下，大概需要掌握如下的知识面，有一些已经忘记了，有一些还没有看过，打算回炉重造一遍，总结的可能不全面，会不断的更新，并同步个人的学习进度，希望技术方面能够有一个大的飞跃。 Javajvm jvm内存区域 垃圾回收与内存分配 jvm类加载 jvm内存模型 jvm性能优化 参考：深入理解java虚拟机（此书为jdk1.7的，此书结合1.8官方文档）, java性能优化权威指南, 揭秘Java虚拟机：JVM设计原理与实现 spring/boot 使用 ioc源码 aop源码 mvc源码 参考：spring源码，spring实战 dubbo dubbo协议与hessian序列化 spi扩展点加载 线程池优化 服务引用 服务发布 rpc通信设计 服务注册 分布式事务 zookeeper 参考：dubbo官方文档，dubbo源码，分布式一致性原理与实践 spring cloudmysql sql语句 Mysql Innodb索引原理 mysql事务和锁 分区分表分库 参考：高性能mysql，mysql官方文档，mysql技术内幕-inoodb存储引擎 redis redis命令 redis数据结构 jedis lua redis源码（选） 参考书籍：redis实战，redis设计与实现 并发 多线程 线程池 锁 线程通信 定时器 并发容器、框架 相关源码（选） 参考：java多线程编程核心技术，java并发编程的艺术 mybatis mybatis使用 mybatis源码 参考：深入浅出MyBatis技术原理与实战，mybatis源码 java基础 参考： java 8 lambda 流 相关源码（选） 参考：java 8实战，写给大忙人看的Java SE 8 算法 参考：剑指offer,刷letcode netty/nio 参考：netty实战，netty权威指南 mq/kafkatomcat 参考：tomcat权威指南 maven Maven入门指南 参考：maven实战，官方文档 git 参考：git版本控制管理 设计模式领域建模微服务架构机器学习前端","categories":[{"name":"杂记","slug":"杂记","permalink":"peachey.blog/categories/杂记/"}],"tags":[]},{"title":"Hessian Lite序列化简析","slug":"java-dubbo-hessian","date":"2018-05-04T14:55:33.000Z","updated":"2018-09-09T18:38:49.233Z","comments":true,"path":"2018/05/04/java-dubbo-hessian/","link":"","permalink":"peachey.blog/2018/05/04/java-dubbo-hessian/","excerpt":"","text":"dubbo是一个专注于服务治理的高性能rpc框架，支持服务之间的远程调用，Hessian序列化的存在是服务间沟通的桥梁，以下是本人阅读各种资料以及源码，对hessian序列化的一点整理与理解，如有错误请指出。 目录 Hessian Lite序列化简介 Dubbo Hessian Lite源码解析2.1 Hessian Lite类结构2.2 Dubbo Hessian Lite源码解析2.3 序列化器和反序列化器 Hessian Lite序列化详述3.1 序列化类型3.2 序列化详述 序列化协议对比 参考 1. Hessian2序列化hessian是一个动态类型,二进制序列化,也是网络协议,为了对象的定向传输,hessian协议有以下的设计目标： 它必须支持描述序列化的类型，即不需要外部架构和接口定义; 它必须是跨语言的，要支持包括脚本语言; 它必须是可以通过单一方式进行读写; 它要尽可能的简洁; 它必须是简单的，它可以有效地测试和实施; 尽可能的快; 必须要支持Unicode编码; 它必须支持八位二进制数据; 它必须支持加密,压缩,签名,还有事务的上下文。 Hessian是二进制传输协议，不需要其他的附加信息，因此是一种很好的能够支持跨语言特性，但是不同的语言需要一套Hessian的实现。 2. Dubbo Hessian Lite源码解析 2.1 Hessian Lite类结构 以上类图是Dubbo Hessian序列化的类结构图，以下对核心类进行讲解： Serialization:此接口定义了序列化方式的规格，Dubbo中每种序列化方式都需要实现此接口定义的方式，此接口持有四个方法：getContentTypeId是获取序列化类型id,此值在dubbo协议头里边也会用到; getContentType此方法是获取序列化类型，一种定义的字符串，比如Hessian是x-application/hessian2,在框架中也没有具体的调用地方; serialize此方法用于序列化; deserialize此方法用于反序列化。 Hessian2Serialization: 这个类是外部调用序列化的入口，这个类实现了Serialization接口, serialize和deserialize是返回新建序列化处理对象，比如return new Hessian2ObjectOutput(out);，通过此种方式保证线程安全调用。 Hessian2Input/Hessian2Output: 这两个类是序列化的核心调度类，所有类型的序列化都是通过该类的方法来实现的，此两个类会依赖Hessian2SerializerFactory此类去选择相应的Serializer和Deserializer,然后调用相应方法进行序列化或反序列化。 Hessian2SerializerFactory: 这个是Serializer和Deserializer的工厂类，继承了SerializerFactory,大部分实现都在SerializerFactory类中，类内部持有静态Map，以类型为KEY, 存储Serializer或Deserializer（value），并且对外提供了两个方法getSerializer和getDeserializer分别用于获取相应的Serializer和Deserializer。 AbstractSerializer/AbstractDeserializer: 这两个类分别实现了Serializer和Deserializer，是具体的序列化器和反序列化器的父抽象类，从此类衍生出了很多具体类型的序列化器和反序列化器，类图中未完全给出，包括一些java 8类型的序列化器和反序列化器。 2.2 Hessian Lite序列化和反序列化流程 Hessian Lite序列化流程如上时序图所述，之前的类图上没有加入DubboCodec，是因为这个不是本文所要分享的内容，与dubbo的rpc通信相关；时序图中描述序列化对象，对象只是一个抽象的称呼，可以是Object、List、Map等等,DubboCodec是对ExchangeCodec的一个继承，并重写了部分方法，其中对于协议头的序列化实现在ExchangeCodec中完成的，DubboCodec中会序列化一些附加信息； 通过Hessian2Serialization获取一个Hessian2ObjectOutput对象，并利用Hessian2ObjectOutput执行序列化操作，Hessian2ObjectOutput内部持有一个Hessian2Objectput变量，具体的序列化操作是实现在Hessian2Objectput, Hessian2Objectput是序列化的核心调度类，其内部会根据具体的类型通过Hessian2SerializerFactory选择相应的序列化器来完成Java对象到二进制数据的转化，二进制数据会保存在Hessian2Objectput内部的buffer中。 Hessian Lite反序列化流程如上时序图所述，DubboCodec需要区分是请求还是响应，对于请求利用DecodeableRpcInvocation来做处理，对于响应利用DecodeableRpcResult来做处理，其实序列化也是区分响应还是请求的，对于序列化DubboCodec是通过两个不同的函数实现的； 在DecodeableRpcInvocation或DecodeableRpcResult中还是一样的，获取到Hessian2ObjectInput, 通过Hessian2ObjectInput内部持有的Hessian2Input完成序列化， Hessian2Input是Hessian2反序列化的一个核心调度类，其内部会根据具体的类型通过Hessian2SerializerFactory选择相应的反序列化器来进行反序列化的，DubboCodec最后会将DecodeableRpcInvocation或DecodeableRpcResult放入向上返回的Request或者Response中； 有必要提一下无论是序列化还是反序列化获取ObjectOutput或ObejctInput都是通过一个CodecSupport的工具类完成的，CodecSupport先确定Serialization（hessian或者其他），然后再通过Serialization来获取ObjectOutput或ObejctInput。 2.3 序列化器和反序列化器2.3.1 选择序列化器或反序列化器当序列化或反序列化的时候会选择与该类型匹配的序列化器，此过程主要是在SerializerFactory该类中完成，此类继承抽象类AbstractSerializerFactory,对外有两个方法getSerializer和getDeserializer，此二方法的作用是通过Class&lt;?&gt;来获取相应的序列化器或反序列化器, 流程如下，此流程只是图形化一下Dubbo中的代码逻辑，并非一定要按照如此优先级进行。 静态Map在类初始化时，以Class为key,Serializer为value,进行了一系列的put操作,其中涉及类型void,Boolean,Byte,Short,Integer,Long,Float,Double,Character, String, Object, java.util.Date, boolean, byte, short, int, long, float, double, char, boolean, byte, short, int, long, float, double, char, String, Object, Class, BigDecimal, File, ObjectName, java.sql.Date, java.sql.Time, java.sql.Timestamp , java.io.InputStream, java.time.LocalTime, java.time.LocalDate, java.time.LocalDateTime, java.time.Instant, java.time.Duration, java.time.Period, java.time.Year, java.time.YearMonth, java.time.MonthDay, java.time.OffsetDateTime, java.time.ZoneOffset, java.time.OffsetTime, java.time.ZonedDateTime,此步操作是通过具体类型来获取序列化器； 缓存Map是当从静态Map中获取不到时，会后续的操作，确定序列化器后会将之放入缓存Map当中，此操作可以看成是一种提高性能的做法。 其他工厂是加载其他的工厂类来获取相应的序列化器，不过debug到这个地方，装载工厂类的Map是空，具体作用待定。 writeReplace与readResolve的作用一样，如果序列化的对象具有此方法，会利用此方法返回的实例来代替序列化后实例，用以保证对象的单例性。 后续的其他类型会判断传入的类型是否为与其类型一致，或者为其子类，来获取相应的序列化器。 默认的序列化器JavaSerializer是用以处理以上类型未包含的类型，比如自定义的对象类型，这个官方的Hessian有点区别，官方的Hessian的默认序列化器是UnsafeSerializer,这个与JavaSerializer的区别是先将对象序列化成Map,同样反序列化时候UnsafeDeserializer先将二进制数据序列化成Map，然后再将Map转化成对象，而JavaDeserializer会新建一个对象然后再把属性设置进去。 选择反序列化器之前的步骤和上述差不多，值得一提的反序列化Class类型时为何不将之放入静态Map当中，是因为改反序列化器需要获取当前的类加载器，而且在流程中还少了一些类型，是因为在初始化静态Map时，将其及其子类已经放入了静态Map当中。 详细实现请参见：com.alibaba.com.caucho.hessian.io.SerializerFactory 2.3.2 部分序列化器和反序列化器介绍BasicSerializer/BasicSerializer该序列化器和反序列化器负责序列化的类型是：null, Boolean/boolean, byte/Byte, short/Short, int/Integer, long/Long, float/Float, double/Double, char/Character, String, 以上类型的数组，Number, Object, Date等，其中byte/Byte, short/Short, int/Integer都是按照整型来序列化与反序列化，float/Float, double/Double都是按照双精度浮点型来序列化与反序列化。 ClassDeserializer/ClassSerializer该序列化器和反序列化器负责序列化的类型是:Class, 对此执行序列化时，是将java.lang.Class以及对象的全类名以字符串的序列化，当反序列化时会根据类的全类名利用Class.forName来加载Class。 StringValueSerializer/StringValueDeserializer该序列化器和反序列化器负责序列化的类型是:BigDecimal, ObjectName, 执行序列化时，会将BigDecimal格式化成字符串, 反序列化时再将字符串格式化成BigDecimal，这也是BigDecimal序列化精度不丢失的原因，不过相对而言序列化性能会稍微差点，ObjectName是JMX MBean要注册的对象名称，此处不做过多介绍。 SqlDateSerializer/SqlDateDeserializer该序列化器和反序列化器负责序列化的类型是:java.sql.Date, 与Date序列化一样，会将通过long型时间戳作为中间变量进行序列化和反序列化。 InputStreamSerializer/InputStreamDeserializer该序列化器和反序列化器负责序列化的类型是:InputStream, 具体的序列化过程是将InputStream中分块将写入Hessian2Output中的buffer当中，在反序列化时候再分块读取。 MapSerializer/MapDeserializer该序列化器和反序列化器负责序列化的类型是:Map及其子类, 具体序列化时候会写入Map头，有类型的写入标识M, 无类型的写入标识H，然后再迭代序列化每个对象，最后再Map的尾标识Z。 CollectionSerializer/CollectionDeserializer该序列化器和反序列化器负责序列化的类型是:Collection及其子类, 比如List、Set等等，具体序列化时候会写入Collection头标识，然后再迭代序列化每个对象，最后再Collection的尾标识，具体序列化细节会在下边详述。 EnumSerializer/EnumDeserializer该序列化器和反序列化器负责序列化的类型是:Enum及其子类，具体序列化过程会将枚举类全类名以及枚举值以字符串形式序列化，在反序列化过程中会用反射的方式来构建枚举对象。 ArraySerializer/ArrayDeserializer该序列化器和反序列化器负责序列化数组类型，其序列化过程是Collection序列化的一种情况，是按照定长列表形式来序列化的。 JavaSerializer/JavaDeserializer该序列化器和反序列化器是默认的，负责序列化的类型是:自定义的对象类型、异常类型等等，其内部持有相应的FieldSerializer数组用来序列化对象中的基本类型字段，每种FieldSerializer内部序列化以及其他对象类型还是调用Hessian2Output相应的序列化方法，反序列化也有类似的过程。 3 Hessian Lite序列化详述 3.1 序列化类型hessian对象序列化有8个基本类型： 原始二进制数据 布尔 64位的毫秒日期类型 64位double类型 32位int类型 64位long类型 null UTF-8编码的字符串 有3中可遍历的类型： 列表(List)：lists and arrays 映射(Map)：maps and dictionaries 对象(Object): objects 同时还有3个内置的引用映射： 对象/列表引用映射 类定义引用映射 类型引用映射（类名） 3.2 序列化详述 语法是hessian标准的语法 二进制数据（byte array）Hessian2语法123binary ::= b b1 b0 &lt;binary-data&gt; binary ::= B b1 b0 &lt;binary-data&gt; ::= [x20-x2f] &lt;binary-data&gt; Hessian Lite二进制数据是以块进行编码的，B表示最后一块，b表示非最后一块，b1表示长度的高八位，b0表示长度的第八位, 长度为一个字节是用l表示： 非最后一块：利用A标识，这是dubbo与标准不同的地方，并用两个字节表示长度, 占用3个字节. 最后一块: 利用B标识，并用两个字节表示长度，占用3个字节. [0, 0x0f]: 当前非最后一块切分剩下的部分的长度在此范围内，长度形式为0x20+b0，占用一个字节 [0x10, 0x3ff]: 当前非最后一块切分剩下的部分的长度在此范围内，长度形式为0x34+b1 b0，占用两个字节 序列化实现详细代码请阅读com.alibaba.com.caucho.hessian.io.Hessian2Output#writeBytes(byte[], int, int) 布尔Hessian2语法12boolean ::= T ::= F Hessian LiteF表示false, T表示true，直接利用字节序列化. 序列化实现详细代码请阅读com.alibaba.com.caucho.hessian.io.Hessian2Output#writeBoolean 日期Hessian2语法12date ::= x4a b7 b6 b5 b4 b3 b2 b1 b0 ::= x4b b4 b3 b2 b1 b0 用距离1970-01-01 00:00:00的一个64位长的毫秒值来表示日期 Hessian Lite日期的序列化方式有两种，如果能够精确到分，可以用32位整型来表示，否则只能用64位整型来表示，0x4b表示32位形式，0x4a表示64位形式, 然后将值移位序列化为二进制. 序列化实现详细代码请阅读com.alibaba.com.caucho.hessian.io.Hessian2Output#writeUTCDate DoubleHessian2语法123456double ::= D b7 b6 b5 b4 b3 b2 b1 b0 ::= x5b ::= x5c ::= x5d b0 ::= x5e b1 b0 ::= x5f b3 b2 b1 b0 Hessian Lite在序列化Double时，是分了几种类型，每种序列化的方式都是要将Double转成整型，然后利用移位填充每个字节： 0x5b代表Double.Zero 0x5c代表Double.One 0x5d代表大小精确到一个字节表示的Double类型 0x5e代表大小精确到两个个字节表示的Double类型 0x5f代表只有3位小数以内的Double类型 默认是8个字节表示，D位Double的标识 为什么要分这么多种方式，是一种提高性能的方式，如果数值很小的话，用8个字节表示，首先会增大传输数据长度，其次8个字节是遵循ieee754标准的，运算过程会相对复杂一点, 并且dubbo中float类型利用序列双精度浮点数类型来处理，序列化实现详细代码请阅读com.alibaba.com.caucho.hessian.io.Hessian2Output#writeDouble IEEE754 双精度浮点数Double.doubleToLongBits这个函数底层调用的是java.lang.Double#doubleToRawLongBits,这是一个native方法,看不到具体的实现方式，但是是ieee745标准的一个实现，以下对ieee754标准的双精度浮点数做一个原理做一个概述性的介绍。双精度浮点数的IEEE754格式如下： 123-----------------------------------------------------------------------------| 符号 1bit | 幂 11bit | 分数 52 bit |----------------------------------------------------------------------------- 符号1位：0表示此值为正，1表示此值为负数 幂11位：指数域的编码值为指数的实际值加上某个固定的值，该固定值为 2^(e-1)-1 分数52位：也就是指转化为科学计数法后的小数部分 举个例子，18.9转换成此种形式： 18.9为正数，所以符号位为0 log2(18.9)=4.24,则指数为可以看做是二进制的4次方，也就是1023+4=1027为10000000011 18.9/2^4=1.18125,注意默认将1去掉，则将0.18125转化为小数二进制为0010111001100110011001100110011001100110011001100110(怎么小数转换成二进制情百度) 然后拼起来便是一个完整的64位双精度的表示方式 还有一些特殊形式，比如+0,-0,NaN等，此处不做多讲述，想了解的可以戳小标题（维基百科，需要翻墙） 整型（int）Hessian2语法1234int ::= &apos;I&apos; b3 b2 b1 b0 ::= [x80-xbf] ::= [xc0-xcf] b0 ::= [xd0-xd7] b1 b0 Hessian Lite为了提高序列化效率，整型分类几种类型去处理, 用v来表示要序列化的值, 形式并按字节顺序排列, b3, b2, b1, b0分别表示长度的各个8位： [-0x10, 0x2f]: 如果值大小位于此区间，占用1个字节，形式为b0+0x90. [-0x800, 0x7ff]: 如果值大小位于此区间，占用2个字节，形式为b1+0xc8 b0. [-0x40000, 0x3ffff]: 如果值大小位于此区间，占用3个字节, 形式为b2+0xd4 b1 b0. 除上述以上情况外，序列化需要占用9个字节，形式I b3 b2 b1 b0, I是整型的标识. dubbo中byte,short类型利用序列化整型处理，序列化实现详细代码请阅读com.alibaba.com.caucho.hessian.io.Hessian2Output#writeInt 长整型（long）Hessian2语法12345long ::= L b7 b6 b5 b4 b3 b2 b1 b0 ::= [xd8-xef] ::= [xf0-xff] b0 ::= [x38-x3f] b1 b0 ::= x4c b3 b2 b1 b0 Hessian Lite长整型在序列化时候与双精度浮点型类似，为了提高序列化效率，也是分类几种类型去处理, b7, b6, b5, b4, b3, b2, b1, b0分别表示长度的各个8位： [-0x08, 0x0f]: 如果值大小位于此区间，占用1个字节，形式为b0+0xe0. [-0x800, 0x7ff]: 如果值大小位于此区间，占用2个字节，形式为b1+0xf8 b0. [-0x40000, 0x3ffff]: 如果值大小位于此区间，占用3个字节, 形式为b2+0x3c b1 b0. [-0x80000000, 0x7fffffff]: 如果值大小位于此区间，占用5个字节，形式0x59 b3 b2 b1 b0 , 此处的前缀0x59是dubbo与标准hessian的一个区别 除上述以上情况外，序列化需要占用9个字节，形式L b7 b6 b5 b4 b3 b2 b1 b0, L是长整型的标识. 序列化实现详细代码请阅读com.alibaba.com.caucho.hessian.io.Hessian2Output#writeLong nullHessian2语法1null ::= N Hessian Litenull的语法与boolean都很简单利用字符N占用一个字节来表示. 序列化实现详细代码请阅读com.alibaba.com.caucho.hessian.io.Hessian2Output#writeNull UTF-8编码的字符串Hessian2语法1234string ::= x52 b1 b0 &lt;utf8-data&gt; string ::= S b1 b0 &lt;utf8-data&gt; ::= [x00-x1f] &lt;utf8-data&gt; ::= [x30-x33] b0 &lt;utf8-data&gt; Hessian Lite字符串会按照块来序列化，如果是最后一块用S来标识，如果不是最后一块利用R来标识, b1, b0分表表示长度高低8位： 非最后一块：利用R标识，并用两个字节来表示长度 [0x00-0x1f]: 如果划分完所有的非最后一块，剩下的部分长度在此范围值内，则不用S来标识，而是直接序列化长度值. [0x20-0x3ff]: 如果划分完所有的非最后一块，剩下的部分长度在此范围值内，则不用S来标识， 序列化形式为0x30+b1 b0,所以长度第一个字节范围[x30-x33] 否则序列化利用S来标识最后一块，并用两个字节来标识长度 dubbo中char类型利用序列化字符串方式来处理，序列化实现详细代码请阅读com.alibaba.com.caucho.hessian.io.Hessian2Output#writeString(java.lang.String) 列表(List)Hessian2语法123456list ::= x55 type value* &apos;Z&apos; # variable-length list ::= &apos;V&apos; type int value* # fixed-length list ::= x57 value* &apos;Z&apos; # variable-length untyped list ::= x58 int value* # fixed-length untyped list ::= [x70-77] type value* # fixed-length typed list ::= [x78-7f] value* # fixed-length untyped list Hessian Lite 详细代码请阅读 映射(Map)Hessian2语法1map ::= M type (value value)* Z Hessian Lite对象(Object)Hessian2语法1234class-def ::= &apos;C&apos; string int string*object ::= &apos;O&apos; int value* ::= [x60-x6f] value* Hessian Lite引用Hessian2语法1ref ::= x51 int Hessian Lite 详细代码请阅读 序列化协议对比 参考 dubbo官方文档 hessian序列化协议 dubbo源码","categories":[{"name":"Java","slug":"Java","permalink":"peachey.blog/categories/Java/"}],"tags":[{"name":"Dubbo","slug":"Dubbo","permalink":"peachey.blog/tags/Dubbo/"}]},{"title":"Innodb索引原理","slug":"db-innodb-index","date":"2018-03-21T14:41:25.000Z","updated":"2018-05-05T09:18:25.000Z","comments":true,"path":"2018/03/21/db-innodb-index/","link":"","permalink":"peachey.blog/2018/03/21/db-innodb-index/","excerpt":"","text":"Innodb是Mysql最常用的存储引擎，了解Innodb存储引擎的索引对于日常工作有很大的益处，索引的存在便是为了加速数据库行记录的检索。以下是我对最近学习的知识的一些总结，以及对碰到的以及别人提到过的问题的一些分析，如有错误，请指正，我会及时更正。 目录 Innodb表结构 B树与B+树 聚簇索引和二级索引 sql执行顺序 sql优化建议 一些问题分析 参考资料 1. Innodb表结构 此小结与索引其实没有太多的关联，但是为了便于理解索引的内容，添加此小结作为铺垫知识。 1.1 Innodb逻辑存储结构Mysql表中的所有数据被存储在一个空间内，称之为表空间，表空间内部又可以分为段(segment)、区(extent)、页(page)、行(row),逻辑结构如下图： 段(segment) 表空间是由不同的段组成的，常见的段有：数据段，索引段，回滚段等等，在Mysql中，数据是按照B+树来存储，因此数据即索引，因此数据段即为B+树的叶子节点，索引段为B+树的非叶子节点,回滚段用于存储undo日志，用于事务失败后数据回滚以及在事务未提交之前通过undo日志获取之前版本的数据，在Innodb1.1版本之前一个Innodb,只支持一个回滚段，支持1023个并发修改事务同时进行，在Innodb1.2版本，将回滚段数量提高到了128个，也就是说可以同时进行128*1023个并发修改事务。 区(extent) 区是由连续页组成的空间，每个区的固定大小为1MB,为保证区中页的连续性，Innodb会一次从磁盘中申请4~5个区，在默认不压缩的情况下，一个区可以容纳64个连续的页。但是在开始新建表的时候，空表的默认大小为96KB,是由于为了高效的利用磁盘空间，在开始插入数据时表会先利用32个页大小的碎片页来存储数据，当这些碎片使用完后，表大小才会按照MB倍数来增加。 页(page) 页是Innodb存储引擎的最小管理单位，每页大小默认是16KB，从Innodb 1.2.x版本开始，可以利用innodb_page_size来改变页size，但是改变只能在初始化Innodb实例前进行修改，之后便无法进行修改，除非mysqldump导出创建新库，常见的页类型有：数据页、undo页、系统页、事务数据页、插入缓冲位图页、插入缓冲空闲列表页、未压缩的二进制大对象页、压缩的二进制大对象页。 行(row) 行对应的是表中的行记录，每页存储最多的行记录也是有硬性规定的最多16KB/2-200,即7992行（16KB是页大小，我也不明白为什么要这么算,据说是内核定义） 1.2 Innodb行记录格式 Innodb提供了两种格式来存储行记录：Redundant格式、Compact格式、Dynamic格式、Compressed格式，Redudant格式是为了兼容保留的。 Redundant行格式（5.0版本之前的格式） 字段长度偏移列表：存储字段偏移量，与列字段顺序相反存放，若列长度小于255字节，用一个字节表示，若大于255字节，用两个字节表示 记录头信息：固定用6字节表示，具体含义如下： 隐藏列：事务id和回滚列id,分别占用6、7字节，若此表没有主键，还会增加6字节的rowid列。 Compact行格式(5.6版本的默认行格式) 变长字段长度列表：此字段标识列字段的长度，与列字段顺序相反存放，若列长度小于255字节，用一个字节表示，若大于255字节，用两个字节表示，这也是Mysql的VARCHAR类型最大长度限制为65535 NULL标志位：标识改列是否有空字段，有用1表示，否则为0，该标志位长度为ceil(N/8)（此处是Mysql技术内幕-Innodb存储引擎与官方文档有出入的地方）； 记录头信息：固定用5字节表示，具体含义如下： 列数据：此行存储着列字段数据，Null是不占存储空间的； 隐藏列：事务id和回滚列id,分别占用6、7字节，若此表没有主键，还会增加6字节的rowid列。 Note:关于行溢出，即Redundant格式、Compact格式存储很长的字符串，在该字段会存储该字符串的前768个字节的前缀（字段超过768字节则为变长字段），并将整个字符串存储在uncompress blob页中。 Dynamic格式(5.7版本默认行格式)和Compressed格式Dynamic格式和Compressed格式与Compact的不同之处在于对于行溢出只会在该列处存放20字节的指针，指向该字符串的实际存储位置，不会存储768字节前缀，而且Compressed格式在存储BLOB、TEXT、VARCHAR等类型会利用zlib算法进行压缩，能够以很高的存储效率来存储字符串。 1.3 Innodb数据页结构《Mysql技术内幕-Innodb存储引擎》书中对此有描述，但是应该不是太准确，书中有如下描述，此处不做详细介绍，若有兴趣请看此神书。 2. B树与B+树 B树与B+树通常用于数据库和操作系统的文件系统中。NTFS, ReiserFS, NSS, XFS, JFS, ReFS 和BFS等文件系统都在使用B+树作为元数据索引。B+ 树的特点是能够保持数据稳定有序，其插入与修改拥有较稳定的对数时间复杂度。 2.1 B树定义：B树（B-TREE）满足如下条件，即可称之为m阶B树： 每个节点之多拥有m棵子树； 根结点至少拥有两颗子树（存在子树的情况下); 除了根结点以外，其余每个分支结点至少拥有 m/2 棵子树； 所有的叶结点都在同一层上； 有 k 棵子树的分支结点则存在 k-1 个关键码，关键码按照递增次序进行排列； 关键字数量需要满足ceil(m/2)-1 &lt;= n &lt;= m-1； B树插入 B树删除 2.2 B+树定义：B+树满足如下条件，即可称之为m阶B+树： 根结点只有一个，分支数量范围为[2，m] 分支结点，每个结点包含分支数范围为[ceil(m/2), m]； 分支结点的关键字数量等于其子分支的数量减一，关键字的数量范围为[ceil(m/2)-1, m-1]，关键字顺序递增； 所有叶子结点都在同一层； 插入：B+树的插入必须保证插入后叶节点中的记录依然排序，同时需要考虑插入B+树的三种情况，每种情况都可能会导致不同的插入算法，插入算法入下图： 插入举例(未加入双向链表)： 1、 插入28这个键值，发现当前Leaf Page和Index Page都没有满，直接插入。 2、 插入70这个键值，Leaf Page已经满了，但是Index Page还没有满，根据中间的值60拆分叶节点。 3、 插入记录95，Leaf Page和Index Page都满了，这时需要做两次拆分 4、 B+树总是会保持平衡。但是为了保持平衡，对于新插入的键值可能需要做大量的拆分页（split）操作，而B+树主要用于磁盘，因此页的拆分意味着磁盘数据移动，应该在可能的情况下尽量减少页的拆分。因此，B+树提供了旋转（rotation）的功能。旋转发生在Leaf Page已经满了、但是其左右兄弟节点没有满的情况下。这时B+树并不会急于去做拆分页的操作，而是将记录移到所在页的兄弟节点上。通常情况下，左兄弟被首先检查用来做旋转操作，在第一张图情况下，插入键值70，其实B+树并不会急于去拆分叶节点，而是做旋转，50，55，55旋转。 删除：B+树使用填充因子（fill factor）来控制树的删除变化，50%是填充因子可设的最小值。B+树的删除操作同样必须保证删除后叶节点中的记录依然排序，同插入一样，B+树的删除操作同样需要考虑下图所示的三种情况，与插入不同的是，删除根据填充因子的变化来衡量。 删除示例(未加入双向链表)：1、删除键值为70的这条记录，直接删除（在插入第三点基础上的图）。 2、接着我们删除键值为25的记录，该值还是Index Page中的值，因此在删除Leaf Page中25的值后，还应将25的右兄弟节点的28更新到Page Index中。 3、删除键值为60的情况，删除Leaf Page中键值为60的记录后，填充因子小于50%，这时需要做合并操作，同样，在删除Index Page中相关记录后需要做Index Page的合并操作。 B树与B+树区别：以m阶树为例： 关键字不同：B+树中分支结点有m个关键字，其叶子结点也有m个，但是B树虽然也有m个子结点，但是其只拥有m-1个关键字。 存储位置不同：B+树非叶子节点的关键字只起到索引作用，实际的关键字存储在叶子节点，B树的非叶子节点也存储关键字。 分支构造不同：B+树的分支结点仅仅存储着关键字信息和儿子的指针，也就是说内部结点仅仅包含着索引信息。 查询不同（稳定）：B树在找到具体的数值以后，则结束，而B+树则需要通过索引找到叶子结点中的数据才结束，也就是说B+树的搜索过程中走了一条从根结点到叶子结点的路径。 3. 聚簇索引和二级索引3.1 聚簇索引每个Innodb的表都拥有一个索引，称之为聚簇索引，此索引中存储着行记录，一般来说，聚簇索引是根据主键生成的。为了能够获得高性能的查询、插入和其他数据库操作，理解Innodb聚簇索引是很有必要的。 聚簇索引按照如下规则创建： 当定义了主键后，innodb会利用主键来生成其聚簇索引； 如果没有主键，innodb会选择一个非空的唯一索引来创建聚簇索引； 如果这也没有，Innodb会隐式的创建一个自增的列来作为聚簇索引。 Note:对于选择唯一索引的顺序是按照定义唯一索引的顺序，而非表中列的顺序, 同时选中的唯一索引字段会充当为主键，或者Innodb隐式创建的自增列也可以看做主键。 聚簇索引整体是一个b+树，非叶子节点存放的是键值，叶子节点存放的是行数据，称之为数据页，这就决定了表中的数据也是聚簇索引中的一部分，数据页之间是通过一个双向链表来链接的，上文说到B+树是一棵平衡查找树，也就是聚簇索引的数据存储是有序的，但是这个是逻辑上的有序，但是在实际在数据的物理存储上是，因为数据页之间是通过双向链表来连接，假如物理存储是顺序的话，那维护聚簇索引的成本非常的高。 3.2 辅助索引除了聚簇索引之外的索引都可以称之为辅助索引，与聚簇索引的区别在于辅助索引的叶子节点中存放的是主键的键值。一张表可以存在多个辅助索引，但是只能有一个聚簇索引，通过辅助索引来查找对应的航记录的话，需要进行两步，第一步通过辅助索引来确定对应的主键，第二步通过相应的主键值在聚簇索引中查询到对应的行记录，也就是进行两次B+树搜索。相反通过辅助索引来查询主键的话，遍历一次辅助索引就可以确定主键了，也就是所谓的索引覆盖，不用回表（查询聚簇索引）。 创建辅助索引，可以创建单列的索引，也就是用一个字段来创建索引，也可以用多个字段来创建副主索引称为联合索引，创建联合索引后，B+树的节点存储的键值数量不是1个，而是多个，如下图： 联合索引的B+树和单键辅助索引的B+树是一样的，键值都是排序的，通过叶子节点可以逻辑顺序的读出所有的数据，比如上图所存储的数据时，按照(a,b)这种形式(1,1),(1,2),(2,1),(2,4),(3,1),(3,2)进行存放，这样有个好处存放的数据时排了序的，当进行order by对某个字段进行排序时，可以减少复杂度，加速进行查询； 当用select * from table where a=? and ?可以使用索引(a,b)来加速查询，但是在查询时有一个原则，sql的where条件的顺序必须和二级索引一致，而且还遵循索引最左原则，select * from table where b=?则无法利用(a,b)索引来加速查询。 辅助索引还有一个概念便是索引覆盖，索引覆盖的一个好处便是辅助索引不高含行记录，因此其大小远远小于聚簇索引，利用辅助索引进行查询可以减少大量的io操作。 4. sql执行顺序 以下的每一步操作都会生成一个虚拟表，作为下一个处理的输入，在这个过程中，这些虚拟表对于用户都是透明的，只用最后一步执行完的虚拟表返回给用户，在处理过程中，没有的步骤会直接跳过。 以下为逻辑上的执行顺序： (1) from：对左表left-table和右表right-table执行笛卡尔积(a*b)，形成虚拟表VT1; (2) on: 对虚拟表VT1进行on条件进行筛选，只有符合条件的记录才会插入到虚拟表VT2中; (3) join: 指定out join会将未匹配行添加到VT2产生VT3,若有多张表，则会重复(1)~(3); (4) where: 对VT3进行条件过滤，形成VT4, where条件是从左向右执行的; (5) group by: 对VT4进行分组操作得到VT5; (6) cube | rollup: 对VT5进行cube | rollup操作得到VT6; (7) having: 对VT6进行过滤得到VT7; (8) select: 执行选择操作得到VT8，本人看来VT7和VT8应该是一样的; (9) distinct: 对VT8进行去重，得到VT9; (10) order by: 对VT9进行排序，得到VT10; (11) limit: 对记录进行截取，得到VT11返回给用户。 Note:on条件应用于连表过滤，where应用于on过滤后的结果（有on的话），having应用于分组过滤 5. sql优化建议 索引有如下有点：减少服务器扫描的数据量、避免排序和临时表、将随机I/O变为顺序I/O。 可使用B+树索引的查询方式 全值匹配：与索引中的所有列进行匹配，也就是条件字段与联合索引的字段个数与顺序相同； 匹配最左前缀：只使用联合索引的前几个字段； 匹配列前缀：比如like &#39;xx%&#39;可以走索引； 匹配范围值：范围查询，比如&gt;,like等； 匹配某一列并范围匹配另外一列：精确查找+范围查找； 只访问索引查询：索引覆盖，select的字段为主键； 范围查询后的条件不会走索引，具体原因会在下一节进行介绍。 列的选择性（区分度）选择性（区分度）是指不重复的列值个数/列值的总个数，一般意义上建索引的字段要区分度高，而且在建联合索引的时候区分度高的列字段要放在前边，这样可以在第一个条件就过滤掉大量的数据，有利用性能的提升，对于如何计算列的区分度，有如下两种方法： 根据定义，手动计算列的区分度，不重复的列值个数/列值的总个数 通过mysql的carlinality,通过命令show index from &lt;table_name&gt;来查看解释一下此处的carlinality并不是准确值，而且mysql在B+树种选择了8个数据页来抽样统计的值，也就是说carlinality=每个数据页记录总和/8*所有的数据页，因此也说明这个值是不准确的，因为在插入/更新记录时，实时的去更新carlinality对于Mysql的负载是很高的，如果数据量很大的话，触发mysql重新统计该值得条件是当表中的1/16数据发生变化时。 但是选择区分度高的列作为索引也不是百试百灵的，某些情况还是不合适的，下节会进行介绍。 Mysql查询过程 当希望Mysql能够高性能运行的时候，最好的办法就是明白Mysql是如何优化和执行的，一旦理解了这一点，很多查询优化工作实际上就是遵循了一些原则让优化器能够按照预想的合理的方式运行————《引用自高性能Mysql》 当想Mysql实例发送一个请求时，Mysql按照如下图的方式进行查询： 客户端先发送一条查询给服务器； 服务器先检查查询缓存，如果命中了缓存，则立刻返回给存储在缓存中的结果，否则进入下一个阶段； 服务器端进行sql解析、预处理，再由优化器生成对应的执行计划； Mysql根据优化器生成的执行计划，调用存储引擎的API来执行查询 将结果返回客户端 注意&amp;建议 主键推荐使用整型，避免索引分裂； 查询使用索引覆盖能够提升很大的性能，因为避免了回表查询； 选择合适的顺序建立索引，有的场景并非区分度越高的列字段放在前边越好，联合索引使用居多； 合理使用in操作将范围查询转换成多个等值查询； in操作相当于多个等值操作，但是要注意的是对于order by来说，这相当于范围查询，因此例如select * from t1 where c1 in (x,x) order by c2的sql是不走索引的； 将大批量数据查询任务分解为分批查询； 将复杂查询转换为简单查询； 合理使用inner join,比如说分页时候 6. 一些问题分析 这个部分是我在学习过程中产生的一些疑问，以及在工作中碰到的或者同事提起的一些问题，对此我做了些调研，总结了一下并添加了些自己的理解，如有错误还请指正。 索引分裂此处提一下索引分裂，就我个人理解，在Mysql插入记录的同时会更新配置的相应索引文件，根据以上的了解，在插入索引时，可能会存在索引的页的分裂，因此会导致磁盘数据的移动。当插入的主键是随机字符串时，每次插入不会是在B+树的最后插入，每次插入位置都是随机的，每次都可能导致数据页的移动，而且字符串的存储空间占用也很大，这样重建索引不仅仅效率低而且Mysql的负载也会很高，同时还会导致大量的磁盘碎片，磁盘碎片多了也会对查询造成一定的性能开销，因为存储位置不连续导致更多的磁盘I/O,这就是为什么推荐定义主键为递增整型的一个原因，Mysql索引页默认大小是16KB，当有新纪录插入的时候，Mysql会留下每页空间的1/16用于未来索引记录增长，避免过多的磁盘数据移动。 自增主键的弊端对于高并发的场景，在Innodb中按照主键的顺序插入可能会造成明显的争用，主键的上界会成为“热点”，因为所有的插入都发生在此处，索引并发的插入可能会造成间隙锁竞争，何为间隙锁竞争，下个会详细介绍；另外一个原因可能是Auto_increment的锁机制，在Mysql处理自增主键时，当innodb_autoinc_lock_mode为0或1时，在不知道插入有多少行时，比如insert t1 xx select xx from t2，对于这个statement的执行会进行锁表，只有这个statement执行完以后才会释放锁，然后别的插入才能够继续执行，,但是在innodb_autoinc_lock_mode=2时，这种情况不会存在表锁，但是只能保证所有并发执行的statement插入的记录是唯一并且自增的，但是每个statement做的多行插入之间是不连接的。 优化器不使用索引选择全表扫描比如一张order表中有联合索引(order_id, goods_id)，在此例子上来说明这个问题是从两个方面来说： 查询字段在索引中 select order_id from order where order_id &gt; 1000,如果查看其执行计划的话，发现是用use index condition,走的是索引覆盖 查询字段不在索引中 select * from order where order_id &gt; 1000, 此条语句查询的是该表所有字段，有一部分字段并未在此联合索引中，因此走联合索引查询会走两步，首先通过联合索引确定符合条件的主键id,然后利用这些主键id再去聚簇索引中去查询，然后得到所有记录，利用主键id在聚簇索引中查询记录的过程是无序的，在磁盘上就变成了离散读取的操作，假如当读取的记录很多时（一般是整个表的20%左右），这个时候优化器会选择直接使用聚簇索引，也就是扫全表，因为顺序读取要快于离散读取，这也就是为何一般不用区分度不大的字段单独做索引，注意是单独因为利用此字段查出来的数据会很多，有很大概率走全表扫描。 范围查询之后的条件不走索引根据Mysql的查询原理的话，当处理到where的范围查询条件后，会将查询到的行全部返回到服务器端（查询执行引擎），接下来的条件操作在服务器端进行处理，这也就是为什么范围条件不走索引的原因了，因为之后的条件过滤已经不在存储引擎完成了。但是在Mysql 5.6以后假如了一个新的功能index condition pushdown(ICP),这个功能允许范围查询条件之后的条件继续走索引，但是需要有几个前提条件： 查询条件的第一个条件需要时有边界的，比如select * from xx where c1=x and c2&gt;x and c3&lt;x,这样c3是可以走到索引的； 支持InnoDB和MyISAM存储引擎； where条件的字段需要在索引中； 分表ICP功能5.7开始支持； 使用索引覆盖时，ICP不起作用。 set @@optimizer_switch = &quot;index_condition_pushdown=on&quot; 开启ICPset @@optimizer_switch = &quot;index_condition_pushdown=off&quot; 关闭ICP 范围查询统计函数不遵循Mysql索引最左原则比如创建一个表：1234567create table `person`( `id` int not null auto_increment primary key, `uid` int not null, `name` varchar(60) not null, `time` date not null, key `idx_uid_date` (uid, time) )engine=innodb default charset=utf8mb4; 当执行select count(*) from person where time &gt; &#39;2018-03-11&#39; and time &lt; ‘2018-03-16’时，time是可以用到idx_uid_date`的索引的,看如下的执行计划： 其中extra标识use index说明是走索引覆盖的，一般意义来说是Mysql是无法支持松散索引的，但是对于统计函数，是可以使用索引覆盖的，因此Mysql的优化器选择利用该索引。 分页offset值很大性能问题在Mysql中，分页当offset值很大的时候，性能会非常的差，比如limit 100000, 20，需要查询100020条数据，然后取20条，抛弃前100000条，在这个过程中产生了大量的随机I/O,这是性能很差的原因，为了解决这个问题，切入点便是减少无用数据的查询，减少随机I/O。解决的方法是利用索引覆盖，也就是扫描索引得到id然后再从聚簇索引中查询行记录，我知道有两种方式： 比如从表t1中分页查询limit 1000000,5 利用inner join select * from t1 inner join (select id from t1 where xxx order by xx limit 1000000,5) as t2 using(id),子查询先走索引覆盖查得id,然后根据得到的id直接取5条得数据。 利用范围查询条件来限制取出的数据 select * from t1 where id &gt; 1000000 order by id limit 0, 5，即利用条件id &gt; 1000000在扫描索引是跳过1000000条记录，然后取5条即可,这种处理方式的offset值便成为0了，但此种方式通常分页不能用，但是可以用来分批取数据。 索引合并1234SELECT * FROM tbl_name WHERE key1 = 10 OR key2 = 20;SELECT * FROM tbl_name WHERE (key1 = 10 OR key2 = 20) AND non_key=30;SELECT * FROM t1, t2 WHERE (t1.key1 IN (1,2) OR t1.key2 LIKE 'value%') AND t2.key1=t1.some_col;SELECT * FROM t1, t2 WHERE t1.key1=1 AND (t2.key1=t1.some_col OR t2.key2=t1.some_col2); 对于如上的sql在mysql 5.0版本之前，假如没有建立相应的联合索引，是要走全表扫描的，但是在Mysql 5.1后引入了一种优化策略为索引合并，可以在一定程度上利用表上的多个单列索引来定位指定行，其原理是将对每个索引的扫描结果做运算，总共有：交集、并集以及他们的组合，但是索引合并并非是一种合适的选择，因为在做索引合并时可能会消耗大量的cpu和内存资源，一般用到索引合并的情况也从侧面反映了该表的索引需要优化。 7. 参考资料 《Mysql技术内幕-Innodb存储引擎》：此书对于Innodb的讲解是比较全面而且细致的，但是稍微有一点点老并且还有一点点错误地方，此书是基于Mysql 5.6版本的，里边会混杂一些5.7的知识。 《MySQL技术内幕：SQL编程》：值得一看。 《高性能Mysql 第三版》：此书是一本Mysql神书，里边有很多的Mysql优化建议以及一些案例 官方文档：这个是比较权威而且是最新的文档，缺点是篇幅很长，内容很多，而且还是纯英文，在理解和阅读速度上相对而言没有中文来得快。","categories":[{"name":"DB","slug":"DB","permalink":"peachey.blog/categories/DB/"},{"name":"Mysql","slug":"DB/Mysql","permalink":"peachey.blog/categories/DB/Mysql/"}],"tags":[{"name":"Mysql","slug":"Mysql","permalink":"peachey.blog/tags/Mysql/"}]},{"title":"Shadowsocks原理详解","slug":"py-shadowsocks","date":"2017-12-28T14:37:56.000Z","updated":"2018-05-04T14:39:44.000Z","comments":true,"path":"2017/12/28/py-shadowsocks/","link":"","permalink":"peachey.blog/2017/12/28/py-shadowsocks/","excerpt":"","text":"Shadowsocks作为一个科学上网工具神器，它的出现极大的便利了Coder or else（本屌丝是这么认为的~）,此神器能够如此流行而又不倒自然是有原因的。Shadowsocks是一种基于Socks5代理方式的网络数据加密传输包，并采用Apache许可证、GPL、MIT许可证等多种自由软件许可协议开放源代码。shadowsocks分为服务器端和客户端，在使用之前，需要先将服务器端部署到服务器上面，然后通过客户端连接并创建本地代理。目前包使用Python、C、C++、C#、Go语言等编程语言开发。以下为本人根据看的各种资料以及略读源码总结而成，有的地方理解也不是很到位，如有错误还望指正。 目录 GFW简介（搜集于互联网） Socks 5协议原理 Shadowsocks工作原理 Shadowsocks部分源码分析 Shadowsocks使用 1. GFW简介（搜集于互联网） GFW为局外人起的绰号，英文Great Firewall of China, 正因为有这东西的存在，导致对外的网络受到其控制，成为了众所周知的“局域网”，这也是大部分翻墙软件的兴起缘由。 1）GFW的重要事件（简单列下，详细点击）： 1998年9月22日，全国公安工作信息化工程――”金盾工程”建设。 2002年9月3日，Google.com被封锁，主要手段为DNS劫持。 2002年9月12日，Google.com封锁解除，之后网页快照等功能被封锁，手段为TCP会话阻断。 2）GFW的主要技术手段 DNS污染/劫持在进行域名访问时，首先会将域名通过dns解析为对应的真实IP，然后通过IP进行HTTP访问，所谓DNS攻击手段，即通过某种手段使得客户机发起DNS查询但得到的却是错误的IP，导致客户机无法正常访问。防火长城会在骨干网出口的53端口进行IDS入侵检测，检测到黑名单域名等，会伪装成域名服务器向客户机发送虚假的回应，由于DNS查询请求一般是基于UDP无连接传输层协议，该协议特征是无状态连接、不可靠传输，DNS查询会接收最先到达的请求，抛弃之后到达的请求，因此导致客户机被欺骗，请求被重定位到虚假IP。 IP封锁在客户机发送请求到服务器的过程中会经过一系列路由的转发，在路由器转发的过程中会根据路由表中存储的表项来决定下一跳的路由器或主机，选择的下一跳地址会根据路由协议来决定。早期使用的是ACL（访问控制列表）来进行IP黑名单限制，现在更高效的路由扩散技术来进行对特定的IP进行封锁。早期路由器都是采用静态路由协议，每一条路由需要进行人工来配置路由表项，或者配置一些策略，在决定路由转发，这时可以通过检测，对相应要封锁的IP配置一条错误的路由，将之牵引到一个不做任何操作的服务器（黑洞服务器），此服务器所要做的就是丢包，这样便无声息封锁掉了。动态路由协议的出现可以更高效的进行屏蔽，动态路由协议可以让路由器通过交换路由表信息来动态更新路由表，并通过寻址算法来决定最优化的路径。因此可以通过动态路由协议的路由重分发功能将错误的信息散播到整个网络，从而达到屏蔽目的。 IP/端口黑名单该手段可以结合上边提到的IP封锁技术，将封锁精确到具体的端口，使该IP的具体端口接收不到请求，从而达到更细粒度的封锁。经常被封锁的端口如下： SSH的TCP协议22端口 HTTP的80端口 PPTP类型VPN使用的TCP协议1723端口，L2TP类型VPN使用的UDP协议1701端口，IPSec类型VPN使用的UDP协议500端口和4500端口，OpenVPN默认使用的TCP协议和UDP协议的1194端口 TLS/SSL/HTTPS的TCP协议443端口 Squid Cache的TCP协议3128端口 无状态TCP连接重置TCP连接会有三次握手，此种攻击方式利用了该特点来进行攻击，gfw会对特定IP的所有数据包进行监控，会对特定黑名单动作进行监控（如TLS加密连接），当进行TCP连接时，会在TCP连接的第二部SYNC-ACK阶段，伪装成客户端和服务器同时向真实的客户端和服务器发送RESET重置，以很低的成本来达到切断双方连接的目的。与丢弃客户机的包相比，在丢包后客户机会不断的发起重试，这样会加重黑洞服务器的负担，利用TCP连接重置来断开连接，客户机也不必发送ACK来确认，这样成本就要低得多。 TCP协议关键字阻断该手段在无状态TCP连接重置手段之上，加入了关键字过滤功能，当协议的头部包含特定的关键字便对其连接进行重置，比如HTTP协议、ED2K协议等等。 深度包检测深度数据包检测（Deep packet inspection,DPI）是一种于应用层对网络上传递的数据进行侦测与处理的技术，被广泛用于入侵检测、流量分析及数据挖掘。就字面意思考虑，所谓“深度”是相对于普通的报文检测而言的——相较普通的报文检测，DPI可对报文内容和协议特征进行检测。基于必要的硬件设施、适宜的检测模型及相应的模式匹配算法，gfw能够精确且快速地从实时网络环境中判别出有悖于预期标准的可疑流量，并对此及时作出审查者所期望的应对措施。 2. SOCKS 5协议原理 SOCKS 5 是一种代理协议，位于应用层于传输层的一个中介层，具体说应该属于会话层。 与SOCKS 4对比： 比SOCKS 4具有更高的安全性，支持更多的加密方式； 支持UDP； 地址方面支持域名和IPV6； SOCKS工作在比HTTP代理更低的层次： SOCKS使用握手协议来通知代理软件其客户端试图进行的连接SOCKS，然后尽可能透明地进行操作，而常规代理可能会解释和重写报头； 同时SOCKS还支持反向代理。 1) SOCK5 工作原理(参考论文)首先大体说一下计算机网络的模型,有助于从总体上明白SOCKS工作原理，如下图 TCP/IP模型 应用层 传输层 网络层 链路层 OSI模型 应用层 展示层 会话层 传输层 网络层 数据链路层 物理层 上边两种模型，这是总所周知的两种计算机网络模型，TCP/IP模型的应用层对应OSI的前三层，网络接入层对应OSI的最后两层，计算机在进行网络连接，请求方会有一个数据封装的过程，接收方会有一个数据解封的过程，具体的流程如下：1234567891011 请求方 接收方应用层 --&gt; data data --&gt; 应用层 | | ∧ ∧ ∨ ∨ | |传输层 --&gt; segment segment --&gt; 传输层 | | ∧ ∧ ∨ ∨ | |网络层 --&gt; package package --&gt; 网络层 | | ∧ ∧ ∨ ∨ | |链路层 --&gt; frame &gt;&gt;传输&gt;&gt; frame --&gt; 链路层 data: 传输层接收的的所有数据，当然包含应用层的协议数据 segment:传输层接收到数据分片，加上TCP/UPD的协议头 package:传输层的分片加上IP头 frame:将IP包前后分别加上帧头与帧尾 SOCK5是属于TCP/IP模型中应用层的协议，因此从以上的网络连接过程，就可以理解基于SOCKS 5协议的请求由客户机到代理机的整个过程如下： 将用户数据添加SOCKS 5头部，发到传输层； 传输层将SOCKS 5协议数据分段，添加TCP/UDP协议数据发到网络层； 网络层将TCP/UDP协议数据添加IP协议头，发往链路层； 链路层添加帧头与尾，将数据封装成帧发往代理机。 2) 基于TCP的SOCKS 5I. SOCKS 5 协商 Note:这个协商其实就是确认客户端与服务端确定验证方式的一次交互。 Client发送身份/方法选择， 协议头格式： VER NMETHODS METHODS VER: 版本，SOCKS 5为0x05 NMETHODS: METHODS部分的长度 METHODS: 是客户端支持的认证方式列表，每个方法占1字节,目前支持如下： 0X00: 不需验证 0x01: GSSAPI 0x02: USERNAME/PASSWORD 0x03~0x7F: IANA分配 0x80~0xFE: 私人方法保留 0xFF: 不接受的方法，也就是未定义/错误 Note: 一般来说具体的实现应该实现GSSAPI与USERNAME/PASSWORD GSSAPI:通用安全服务应用程序层, 能够使程序员在编码过程中实现通用安全，具体来说就是不用针对特定平台、特定安全机制、特定的保护形式以及特定的传输协议去进行安全实现，也就是说程序支持GSSAPI就可以说该程序是满足网络安全的，比如说支持公钥加密形式。 IANA：互联网地址编码分配机构，1)域名。IANA管理DNS域名根和.int，.arpa域名以及IDN（国际化域名）资源.2)数字资源。IANA协调全球IP和AS（自治系统）号并将它们提供给各区域Internet注册机构。3)协议分配。IANA与各标准化组织一同管理协议编号系统。 Server选择方法，协议头格式： VER METHOD VER: 版本， SOCKS 5为0x05 METHOD : Server从Client发送过来的METHODS中选择的方法 Note:接着Client与Server进行具体方法的子协商，但是若Server选择的方法为0xFF, 则Client必须断开连接 II. SOCKS 5 数据传输Client请求，协议头格式: VER CMD RSV ATYP DST.ADDR DST.PROT VER: 协议版本，SOCKS 5为0x05 CMD: CMD是命令码 0x01 CONNECT请求 0x02 BIND请求 0x03 UDP转发 RSV: 0x00 保留字段 ATYP: 地址类型 0x01 IPV4 0x03 域名 0x04 IPV6 DST.ADDR: 目标地址(即想访问的地址) DST.PORT: 目标地址的端口 Note: 代理及会根据DST.ADDR与DST.PORT这两个字段来请求目标主机 关于CMD字段： CONNECT:是指TCP代理模式 BIND:指双向连接，比如FTP协议，一个连接用于发送命令指令，另外一个连接用于传输数据 UDP:指UDP代理模式 Server回应， 协议头格式： VER CMD RSV ATYP BND.ADDR BND.PORT VER: 版本， SOCKS 5为0x05 REP: 回应字段 0x00 成功 0x01 socks服务器错误 0x02 未允许的连接 0x03 网络不可达 0x04 主机不可达 0x05 连接拒绝 0x06 TTL过期 0x07 命令码不支持 0x08 地址类型不支持 0x09~0xFF 未分配 RSV: 0x00 保留字段 ATYP: 地址类型 0x01 IPV4 0x03 域名 0x04 IPV6 BND.ADDR: socks服务器绑定地址 BND.PORT: socks服务器绑定端口 Note:Client会根据BND.ADDR与BND.PORT这两个字段向代理Server发送请求保留字段必须设置为0x00 3) 基于UDP的SOCKS 5基于UDP的请求和响应头是一样的，唯一不同的是Client的请求地址和端口字段是BIND.ADDR和BIND.PORT，格式如下： RSV FRAG ATYP DST.ADDR/BIND.ADDR DST.PORT/BIND.PROT DATA RSV: 0x00 保留字段 FRAG:当前分片ID，0x00表示不分片 ATYP: 地址类型 0x01 IPV4 0x03 域名 0x04 IPV6 DST.ADDR/BIND.ADDR: 目标主机/绑定主机 DST.PORT/BIND.PORT: 目标端口/绑定端口 DATA: 用户数据 Note: 代理及会根据DST.ADDR与DST.PORT这两个字段来请求目标主机,Client会根据BND.ADDR与BND.PORT这两个字段向代理Server发送请求。 FRAG字段用于标识是否进行分片，如果进行分片，数值越大表示分片排序越靠后，如果值为0x00则表示不分片，也就是说分片的order是从1开始的，表示范围为1~127。如果分片的话，每个接收者必须实现一个用于重新组长的队列（REASSEMBLY QUEUE）和一个用于标识过期的计时器（REASSEMBLY TIMER）当有分片被丢弃时，队列应该重新初始化。 地址类型不同，每个UDP报文的大小应该有所限制，以下说明都是方法独立的，按每次来算： ATYP为0x01时，小于10个字节 ATYP为0x02时，小于262个字节 ATYP为0x03时，小于20个字节 3. Shadowsocks工作原理 终于说到Shadowsocks工作原理了，在说这个之前必须先介绍以下普通socks 5的工作原理，将之与Shadowsocks的“变异版”进行对比，就可以看出Shadowsocks处理的妙处了！！ 1) 普通Socks 5 工作原理普通的代理是直接应用Socks 5 协议进行交互，也就是说客户端便是你的主机，Socks 5的Server是远程的代理机，具体的工作模式如下：1Socks 5客户端 &lt;---Socks 5---&gt; Socks 5服务器 &lt;---正常请求---&gt; 目标主机 Socks 5客户端在与Socks 5服务器交互的整个过程是有可能暴露在整个互联网中的，因此很容易被监控到，根据协议特征也可以很容易识别出来，若采取普通的Socks 5代理方式的话，若用于翻墙去看外边的世界，这种方式很容易被墙，代理服务器的IP极容易被加入黑名单，也就导致此代理的寿终正寝，因此一种新的方式Shadowsocks出现了。 2) Shadowsocks Socks 5工作原理Shadowsocks的处理方式是将Socks 5客户端与Socks5服务器的连接提前，Socks5协议的交互完全是在本地进行的，在网络中传输的完全是利用加密算法加密后的密文，这就很好的进行了去特征化，使得传输的数据不是很容易的被特征识别，本人认为这是Shadowsocks的最大的创新了，具体的流程如下：1234567Socks 5客户端 &lt;---Socks 5---&gt; sslocal ∧ | 密文 | ∨ ssserver &lt;---正常请求---&gt; 目标主机 其它方面的处理都与普通代理一样，特殊之处将Socks 5服务器拆成了两个部分： 本地的sslocal：sslocal对于Socks 5客户端便是Socks 5服务器,对于Socks 5客户端是透明的，sslocal完成与Socks 5客户端所有的交互。 远程的ssserver：ssserver对于目标主机同样也是Socks 5服务器，对于目标主机是透明的，完成Socks 5服务器与目标主机的所有操作。 sslocal-ssserver:sslocal接收到Socks 5客户端发送的数据，会将数据加密，并将配置信息发送到ssserver，ssserver接收到配置信息进行权限验证，然后将数据进行解密，然后将明文发往目标主机；当目标主机响应ssserver，ssserver将接收到的数据进行解包，并将数据加密，发送到sslocal，sslocal接收到加密后的数据进行解密，再发送给Socks 5客户端，这就完成了一次交互。 Note:整个流程的关键部分都在内部完成，在网络中传输的都是加密后的密文，很巧妙。 4. Shadowsocks部分源码分析 接下来便是对Shadowsocks源码部分的分析，由于本人对Python不是很精通，同时也没有深入的去研读其代码，代码部分便是略读了一下，在此简单的介绍下其源码，当然要感谢@clowwindy大大。 1) 源码文件结构Shadowsocks有多中语言的版本，Python是原版，整个项目的代码不多，总共4000多行，因此整体看起来并不是很困难，有时间的同学可以仔细去看下，我下载的是2.9.1版本的代码。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798|-shadowsocks-2.9.1| |---- LICENSE| |-debian| | |---- shadowsocks.manpages| | |---- compat| | |---- install| | |---- sslocal.1| | |---- changelog| | |---- init.d| | |---- config.json| | |---- ssserver.1| | |-source| | | `---- format| | |---- docs| | |---- rules| | |---- copyright| | |---- control| | `---- shadowsocks.default| |---- Dockerfile| |---- CHANGES| |-tests| | |---- server-dnsserver.json| | |---- nose_plugin.py| | |---- coverage_server.py| | |---- graceful_cli.py| | |---- graceful_server.py| | |---- aes.json| | |---- server-multi-passwd-empty.json| | |-socksify| | | |---- socks.conf| | | `---- install.sh| | |---- jenkins.sh| | |---- aes-cfb8.json| | |---- test_udp_src.sh| | |---- gen_multiple_passwd.py| | |---- salsa20.json| | |---- test_graceful_restart.sh| | |---- test.py| | |---- client-multi-server-ip.json| | |---- setup_tc.sh| | |---- server-multi-passwd-performance.json| | |---- rc4-md5.json| | |---- test_command.sh| | |---- test_large_file.sh| | |---- chacha20.json| | |---- server-multi-passwd-table.json| | |---- table.json| | |-libsodium| | | `---- install.sh| | |---- ipv6.json| | |---- workers.json| | |---- graceful.json| | |---- chacha20-ietf.json| | |---- fastopen.json| | |---- salsa20-ctr.json| | |---- test_udp_src.py| | |---- server-multi-passwd.json| | |---- ipv6-client-side.json| | |---- rc4-md5-ota.json| | |---- test_daemon.sh| | |---- assert.sh| | |---- server-multi-passwd-client-side.json| | |---- aes-cfb1.json| | |---- server-multi-ports.json| | `---- aes-ctr.json| |---- MANIFEST.in| |-utils| | |-fail2ban| | | `---- shadowsocks.conf| | |---- README.md| | `---- autoban.py| |---- README.md| |---- setup.py| |---- .gitignore| |---- CONTRIBUTING.md| |---- README.rst| |---- .travis.yml| |-shadowsocks| | |---- encrypt.py| | |---- lru_cache.py| | |-crypto| | | |---- openssl.py| | | |---- util.py| | | |---- __init__.py| | | |---- rc4_md5.py| | | |---- sodium.py| | | `---- table.py| | |---- server.py| | |---- local.py| | |---- udprelay.py| | |---- shell.py| | |---- __init__.py| | |---- eventloop.py| | |---- tcprelay.py| | |---- common.py| | |---- manager.py| | |---- asyncdns.py| | `---- daemon.py Shadowsocks的主要代码都在shadowsocks目录下，其他目录提供了一下打包、测试和许可协议的内容，以下简单介绍一下各个文件的内容：123456789101112131415161718192021shadowsocks|---- encrypt.py 提供加密函数调用|---- lru_cache.py 实现LRU缓存，用于应对并发量比较大的状况|-crypto 加密功能包| |---- openssl.py：openssl库调用| |---- util.py：工具类| |---- __init__.py| |---- rc4_md5.py：rc4-md5加密| |---- sodium.py：sodium加密| `---- table.py |---- server.py：ssserver实现|---- local.py：sslocal实现|---- udprelay.py：udp代理方式实现|---- shell.py：shell命令实现|---- __init__.py|---- eventloop.py：事件循环|---- tcprelay.py：tcp代理方式实现|---- common.py：公用类|---- manager.py：管理事件处理、连接处理等等|---- asyncdns.py：异步dns解析`---- daemon.py：守护线程实现 2) 源码分析 接下来分析一下ss的主流程代码，在整体对ss的工作原理有一个深入的了解。 I. local &amp; serverlocal.py中的代码实现的是本地客户端的实现，代码很短，几十行，将日志等对主流程无用代码精简过后，如下：12345678910111213141516171819202122# 加载配置文件config = shell.get_config(True)# 是否运行为守护进程daemon.daemon_exec(config)# 创建异步dns查询对象dns_resolver = asyncdns.DNSResolver()# 创建tcp代理方式转发对象tcp_server = tcprelay.TCPRelay(config, dns_resolver, True)# 创建udp代理方式转发对象udp_server = udprelay.UDPRelay(config, dns_resolver, True)# 创建事件循环处理对象loop = eventloop.EventLoop()# 将dns查询、tcp代理方式转发、udp代理方式转发绑定到事件循环dns_resolver.add_to_loop(loop)tcp_server.add_to_loop(loop)udp_server.add_to_loop(loop)# 预设信号处理函数，接收到正常的退出信号signal.signal(getattr(signal, &apos;SIGQUIT&apos;, signal.SIGTERM), handler)# SIGINT是键盘ctrl+csignal.signal(signal.SIGINT, int_handler)# 开启事件循环loop.run() server.py和local.py的基本流程时差不多的，与local.py不同的地方需要对多个客户端连接过来的请求进行处理，因此会多一些流程，下面是精简过的代码：12345678910111213141516171819202122232425262728293031# 加载配置文件config = shell.get_config(False)# 是否运行为守护进程daemon.daemon_exec(config)# 创建异步dns查询对象dns_resolver = asyncdns.DNSResolver()# 添加tcp代理方式转发tcp_servers.append(tcprelay.TCPRelay(a_config, dns_resolver, False))# 添加udp代理方式转发udp_servers.append(udprelay.UDPRelay(a_config, dns_resolver, False))# 预设信号处理函数，接收到正常的退出信号signal.signal(getattr(signal, &apos;SIGQUIT&apos;, signal.SIGTERM),child_handler)# SIGINT是键盘ctrl+csignal.signal(signal.SIGINT, int_handler)# 创建事件循环处理对象loop = eventloop.EventLoop()# 将dns绑定到事件循环dns_resolver.add_to_loop(loop)# 实现多个线程处理def run_server(): ... loop = eventloop.EventLoop() dns_resolver.add_to_loop(loop) # 开启事件处理无限循环 loop.run() ...if int(config[&apos;workers&apos;]) &gt; 1: ... # 生成多个线程来处理，也就是配置的workerselse: run_server() II. udp &amp; tcpudprelay.py为UDP代理方式的实现，一下为精简后代码，抽取主要流程。123456789101112131415161718192021222324252627282930313233343536SOCKS5 UDP 请求+----+------+------+----------+----------+----------+|RSV | FRAG | ATYP | DST.ADDR | DST.PORT | DATA |+----+------+------+----------+----------+----------+| 2 | 1 | 1 | Variable | 2 | Variable |+----+------+------+----------+----------+----------+SOCKS5 UDP 响应+----+------+------+----------+----------+----------+|RSV | FRAG | ATYP | DST.ADDR | DST.PORT | DATA |+----+------+------+----------+----------+----------+| 2 | 1 | 1 | Variable | 2 | Variable |+----+------+------+----------+----------+----------+shadowsocks UDP 请求 (加密前)+------+----------+----------+----------+| ATYP | DST.ADDR | DST.PORT | DATA |+------+----------+----------+----------+| 1 | Variable | 2 | Variable |+------+----------+----------+----------+shadowsocks UDP 响应 (加密前)+------+----------+----------+----------+| ATYP | DST.ADDR | DST.PORT | DATA |+------+----------+----------+----------+| 1 | Variable | 2 | Variable |+------+----------+----------+----------+shadowsocks UDP 请求和响应 (加密后)+-------+--------------+| IV | PAYLOAD |+-------+--------------+| Fixed | Variable |+-------+--------------+命名：dest 目标主机local ss的本地serverremote ss的远程serverclient 向其他UDPserver发送请求的UDP clientsserver 处理用户请求的UDP server 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465#构造函数，config为配置文件，dns_resolver为dns解析，is_local用于区分是local还是server，stat_callback为回调函数用于统计，在manager.py中有调用def __init__(self, config, dns_resolver, is_local, stat_callback=None): ... #全局参数声明、赋值 #创建socket、绑定端口、设置阻塞方式 server_socket = socket.socket(af, socktype, proto) server_socket.bind((self._listen_addr, self._listen_port)) server_socket.setblocking(False) ...# 获取服务器IP+端口def _get_a_server(self): ...# 关闭连接def _close_client(self, client): ...# 处理serverdef _handle_server(self): # 获取udp server server = self._server_socket # 接收数据 data, r_addr = server.recvfrom(BUF_SIZE) # 解密数据，如果是local的话，因为接收的是本地的数据，是不需要解密的 data, key, iv = encrypt.dencrypt_all(self._password,self._method,data) # 解析头 header_result = parse_header(data) addrtype, dest_addr, dest_port, header_length = header_result ... #如果是local需要将数据发送到remote,如果是remote需要将数据发送到dest if self._is_local: server_addr, server_port = self._get_a_server() else: server_addr, server_port = dest_addr, dest_port ... # 加密数据 data = encrypt.encrypt_all_m(key, iv, m, self._method, data) ... #发送 client.sendto(data, (server_addr, server_port))# 处理clientdef _handle_client(self, sock): # 接收数据 data, r_addr = sock.recvfrom(BUF_SIZE) ... # 加密数据 data = encrypt.encrypt_all(self._password, self._method, 0, data) ... # 获取地址 client_addr = self._client_fd_to_server_addr.get(sock.fileno()) ... # 发送 self._server_socket.sendto(response, client_addr)# 添加事件def add_to_loop(self, loop): ...# 处理事件def handle_event(self, sock, fd, event): # 区分处理client或server if sock == self._server_socket: self._handle_server() elif sock and (fd in self._sockets): self._handle_client(sock)# 周期性处理def handle_periodic(self): ...# 关闭UDP代理def close(self, next_tick=False): tcprelay.py 抽取主流程代理如下：1234567891011121314151617181920212223# 对于每个端口，有一个tcp relay,对于每个tcp relay# 对于每个连接，有一个tcp relay handler来处理# 对于每个handler，有两个socket,# - local 连接到client,# - remote 连接到远程主机（要访问的主机）# 对于每个handler可以有如下几个阶段：# sslocal# - stage 0 接收从本地的method请求，并返回选择信息# - stage 1 从本地接收请求地址，并进行dns解析# - stage 2 udp相关# - stage 3 dns解析完毕，连接远程主机# - stage 4 连接中，并接收本地发送的数据# - stage 5 远程主机连接成功，发送数据# ssserver# - stage 0 直接跳到阶段1# - stage 1 从本地接收请求地址，并进行dns解析# - stage 2 udp相关# - stage 3 dns解析完毕，连接远程主机# - stage 4 连接中，并接收本地发送的数据# - stage 5 远程主机连接成功，发送数据# 对于每个handler，有两个流方向：# upstream：client-&gt;server,读本地，写入远程主机# downstream：server-&gt;client,读远程主机，写入本地 12345678910111213141516171819202122232425262728293031# config为配置，dns_resolver为dns解析，is_local是否为client，stat_callback回调用于统计def __init__(self, config, dns_resolver, is_local, stat_callback=None): ... # 全局参数声明、赋值 # 创建socket、绑定端口、设置阻塞方式 server_socket = socket.socket(af, socktype, proto) server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) server_socket.bind(sa) server_socket.setblocking(False)...# 添加到事件循环def add_to_loop(self, loop): ...# 处理事件def handle_event(self, sock, fd, event): ... # 如果是server_socket 监听 if sock == self._server_socket: ... # 监听 conn = self._server_socket.accept() ... # 如果不是，进行事件处理 else: ... handler.handle_event(sock, event)# 周期性处理def handle_periodic(self): ...# 关闭TCP代理def close(self, next_tick=False): III. eventloopepoll模型（多路复用IO模型） Shadowsocks之前是采用多线程模式，一个连接来了应用一个线程处理，但是这种处理方式在并发量特别大的情况下，性能不是很理想，后来转用eventloop模型，基于epoll模型的一种封装，因此此处需要介绍一下epoll模型。 下边说的是linux下的epoll，epoll_create是创建epoll对象，epoll_ctrl是控制时间函数，比如添加、删除和修改事件等等，epoll_wait是用于返回IO事件，类似ss中的poll函数 在linux，一切皆文件．所以当调用epoll_create时，内核给这个epoll分配一个file，但是这个不是普通的文件，而是只服务于epoll． 所以当内核初始化epoll时，会开辟一块内核高速cache区，用于安置我们监听的socket，这些socket会以红黑树的形式保存在内核的cache里，以支持快速的查找，插入，删除．同时，建立了一盒list链表，用于存储准备就绪的事件．所以调用epoll_wait时，在timeout时间内，只是简单的观察这个list链表是否有数据，如果没有，则睡眠至超时时间到返回；如果有数据，则在超时时间到，拷贝至用户态events数组中． 那么，这个准备就绪list链表是怎么维护的呢？当我们执行epoll_ctl时，除了把socket放到epoll文件系统里file对象对应的红黑树上之外，还会给内核中断处理程序注册一个回调函数，告诉内核，如果这个句柄的中断到了，就把它放到准备就绪list链表里。所以，当一个socket上有数据到了，内核在把网卡上的数据copy到内核中后就来把socket插入到准备就绪链表里了。 epoll有两种模式LT(水平触发)和ET(边缘触发)，LT模式下，主要缓冲区数据一次没有处理完，那么下次epoll_wait返回时，还会返回这个句柄；而ET模式下，缓冲区数据一次没处理结束，那么下次是不会再通知了，只在第一次返回．所以在ET模式下，一般是通过while循环，一次性读完全部数据．epoll默认使用的是LT． epollloop.py:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657# 构造函数,ss支持epoll、kqueue、selectdef __init__(self): if hasattr(select, &apos;epoll&apos;): self._impl = select.epoll() model = &apos;epoll&apos; elif hasattr(select, &apos;kqueue&apos;): self._impl = KqueueLoop() model = &apos;kqueue&apos; elif hasattr(select, &apos;select&apos;): self._impl = SelectLoop() model = &apos;select&apos; ...# 获取事件def poll(self, timeout=None): events = self._impl.poll(timeout) return [(self._fdmap[fd][0], fd, event) for fd, event in events]# 添加时间def add(self, f, mode, handler): fd = f.fileno() self._fdmap[fd] = (f, handler) self._impl.register(fd, mode)# 移除事件def remove(self, f): fd = f.fileno() del self._fdmap[fd] self._impl.unregister(fd)# 定时添加def add_periodic(self, callback): self._periodic_callbacks.append(callback)# 周期性移除def remove_periodic(self, callback): self._periodic_callbacks.remove(callback)# 修改def modify(self, f, mode): fd = f.fileno() self._impl.modify(fd, mode)# 停止def stop(self): self._stopping = True# 运行函数def run(self): events = [] while not self._stopping: asap = False # epoll的系统调用，事件发生，poll调用返回。和select系统调用的关键区别 events = self.poll(TIMEOUT_PRECISION) ... # 循环处理事件 for sock, fd, event in events: handler = self._fdmap.get(fd, None) if handler is not None: handler = handler[1] try: handler.handle_event(sock, fd, event) except (OSError, IOError) as e: shell.print_exception(e) ... 5. Shadowsocks使用 这部分在网上一搜一大片，主要是先得有一个可以联通外边网站的服务器，然后在服务器上搭建ssserver,然后配置本地的sslocal，其实现在有很多的ui客户端，要比命令行好用的多,以下举例用ubuntu搭建ssserver。 I. ssserver搭建 更新软件源：sudo apt-get update 安装pip环境：sudo apt-get install python-pip 安装sserver：sudo pip install shadowsocks II. sserver配置 使用命令启动：sudo ssserver -p 8388 -k password -m aes-256-cfb -d start 使用配置文件：sudo ssserver -c /etc/shadowsocks.json -d start加上-d start可以让服务端在后台运行，停止改为stop即可配置文件： 123456789101112&#123; # 服务器IP &quot;server&quot;:&quot;my_server_ip&quot;, # 代理端口 &quot;server_port&quot;:8388, # 密码 &quot;password&quot;:&quot;mypassword&quot;, # 超时时间 &quot;timeout&quot;:300, # 加密方式 &quot;method&quot;:&quot;aes-256-cfb&quot;&#125; 配置开机启动编辑rc.local文件sudo vi /etc/rc.local在exit 0 这一行之前加入/usr/local/bin/ssserver –c /etc/shadowsocks.json 加速：可以选择kcptun、锐速、finalspeed等插件，kcptun简单粗暴而且免费。","categories":[{"name":"Python","slug":"Python","permalink":"peachey.blog/categories/Python/"}],"tags":[{"name":"Shadowsocks","slug":"Shadowsocks","permalink":"peachey.blog/tags/Shadowsocks/"}]},{"title":"Maven入门指南","slug":"java-maven-introduction","date":"2016-11-12T14:25:44.000Z","updated":"2018-05-05T13:00:41.000Z","comments":true,"path":"2016/11/12/java-maven-introduction/","link":"","permalink":"peachey.blog/2016/11/12/java-maven-introduction/","excerpt":"","text":"在业余时间对Maven的文档做了一个翻译，由于本人的水平有限，可能会有一些出入，欢迎指正。这个指南意欲为第一次使用Maven的人做参考，同时也是作为一个常见用例的指南。对于第一次使用的人，推荐按这个指南的步骤去学习，对于比较熟悉Maven的人来说，这个指南尽量为目前的需求提供一个快速的解决方案。以下的教程是假设读者已经下载Maven并将Maven本地安装的前提下，如果没有进行次步，请点击这个链接下载和安装下载并安装。好了，现在你应该已经安装了Maven,我们即将开始。在我们介绍例子之前，我们先会简介的介绍一下什么是Maven、Maven怎样帮助你做日常工作以及与团队协作。当然,Maven对于小项目很有效，但是Maven同样能够很有效的使团队成员聚焦于项目的更重要的部分，基础架构的建立完全可以交给Maven. 小结目录 什么是Maven？ Mavne如何使我的开发流程受益？ 我如何初始化Maven？ 我如何建立第一个Maven项目？ 我如何编译应用源码？ 我如何编译测试源码并且做单元测试？ 我如何创建一个jar文件并且将之安装到我的本地仓库？ 什么是SNAPSHOT版本？ 我如何使用插件？ 我如何将资源添加到jar文件当中？ 我如何过滤资源文件？ 我如何使用外部的依赖？ 我如何发布jar到远程仓库？ 我如何创建帮助文档？ 我如何构建其他类型的项目？ 我如何同时构建多个项目？ 什么是Maven？大致一看，Maven貌似有很多东西，但是简单来说，Maven是对于模型化创建项目基础结构的一种尝试，通过一种清晰的并且拥有最佳实践效果的目录来提高生产率和项目的可读性。Maven是一款项目管理和理解项目的工具，通过以下方式来管理项目： 构建 帮助文档 报告 依赖 软件配置管理 发行 发布如果你想知道更多关于Maven的背景知识，你可以查阅Maven的哲学以及Maven的历史。接下来开始讲述如何让用户受益于Maven。 Mavne如何使我的开发流程受益？Maven有助于你项目的构建过程，Maven通过使用标准约定和惯例来加速你的开发周期，同时有助于提高你的项目开发的成功率。现在我们概述了一些关于Maven的历史和目的，让我们通过几个实际的例子来帮助你使用Maven! 我如何初始化Maven？Maven的默认配置通常都很高效，但是如果你需要改变缓存位置，或通过HTTP代理使用Maven，你需要创建配置文件，具体请看Maven配置指南 我如何建立第一个Maven项目？我们将要开始创建你的第一个Maven项目！为了创建我们第一个Maven项目，我们需要利用Maven的archetype插件。archetype被定义为原始的类型或者模型，是从所有拥有相似成分的东西抽象出来的。在Maven中，archetype是一个项目模板，包含了一些针对用户需求生成的一些上线的Maven项目的特性。现在我们将要展示archetype插件如何工作，但是如果你想知道更多的关于archetype的信息，请看Archetypes介绍。开始创建你的第一个项目，为了创建最简单的Maven项目，请执行如下命令：1234mvn -B archetype:generate \\ -DarchetypeGroupId=org.apache.maven.archetypes \\ -DgroupId=com.mycompany.app \\ -DartifactId=my-app 一旦你执行了这条命令，你将会发现一些事情会发生。首先你会注意到一个名称为my-app的目录被创建为一个新的项目，冰鞋这个目录包含一个名称为pom.xml的文件，如下：1234567891011121314151617181920&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.mycompany.app&lt;/groupId&gt; &lt;artifactId&gt;my-app&lt;/artifactId&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;name&gt;Maven Quick Start Archetype&lt;/name&gt; &lt;url&gt;http://maven.apache.org&lt;/url&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.11&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; pom.xml包含此项目的项目对象模型(POM).POM是Maven的最小工作单元，这个很重要，需要记住，因为Maven所有的围绕关于项目概念的东西是项目式的，简单来说，POM包含了所有关于你的项目重要的信息片段，而且基本上是以一站式服务的形式来找到所有关于你的项目的的东西。明白POM是很重要的，新手推荐看一下POM简介。上边pom文件描述的是一个很简单的POM，但是仍然展现出了每个POM的关键元素，因此让我们通过他们来了解POM的要点： project:这是所有Maven的pom.xml的顶层元素 modelVersion:这个元素表明当前POM所使用什么版本的对象模型。这个版本号很少发生改变，为了确保使用的稳定性，这是强制的，有但也是很少就是当Maven开发人员觉得有必要改变模型的时候。 groupId:这个元素代表创建这个项目的组织或小组的唯一标识。这个groupId是一个项目的关键标识符，很典型的做法是基于你组织的经过完全认证的域名。比如org.apache.maven.plugins是所有Maven插件的指定groupId。 artifactId：这个元素代表着个项目生成的初级artifact的唯一基名。一个项目的初级artifact通常都是一个jar文件。二级artifacts比如源码包经常会使用artifactId作为他们最终名称的一部分。一个Mavne生成的典型的artifact应该有类似-.的命名模式。(例如myapp-1.0.jar) packaging:这个元素代表打包类型，比如jar,war,ear等等。这不仅意味artifact生成文件是jar,war或者ear，而且指出在构建过程中使用的特定的生命周期。(生命周期这个主题我们将会在指南的接下来进行讲述。现在，只需记住一个项目的打包类型可以在定制的构建生命周期当中发挥作用).packaging元素的默认值是jar，因此你在大多数项目中没有必要去指定。 version:这个元素代表项目生成的artifact的版本。Maven的版本管理对你大有帮助并且你经常在版本号中看到SNAPSHOT，这表明这个项目还处于开发的阶段，我们将会讨论snapshots的使用以及在本指南中其有何进一步功用。 name:这个元素代表项目的显示名称，经常在Maven生成文档的使用被用到。 description:这个元素提供了为你的项目提供了一个基本的描述。这个也经常在Maven生成文档时候被用到。对于适合在POM中使用的元素的完全参考，请参考POM参考，现在让我们返回到项目。在通过archetype生成你的第一个项目后，你会注意到如下的目录结构被创建了：123456789101112131415my-app|-- pom.xml`-- src |-- main | `-- java | `-- com | `-- mycompany | `-- app | `-- App.java `-- test `-- java `-- com `-- mycompany `-- app `-- AppTest.java 正如你看到的，这个项目拥有一个POM和一个你的应用源码的源码目录树以及一个你的测试代码的源码目录树。这个是Maven项目的标准布局(应用源码在${basedir}/src/main/java中，测试代码在${basedir}/src/test/java,${basedir}代表包含pom.xml文件的目录)。如果你准备创建一个maven项目，这中目录结构是我们推荐使用的。这是一个Maven的约定，想要了解更多，你可以阅读标准目录布局介绍。现在我们拥有一个POM，一些应用源码，一些测试代码，你有可能会问… 我如何编译应用源码？将目录改变到pom.xml所在的目录并且执行如下的命令来编译你的应用源码：1mvn compile 在执行了这条命令后你应该看到如下的输出：123456789101112131415161718192021[INFO] ----------------------------------------------------------------------------[INFO] Building Maven Quick Start Archetype[INFO] task-segment: [compile][INFO] ----------------------------------------------------------------------------[INFO] artifact org.apache.maven.plugins:maven-resources-plugin: \\ checking for updates from central...[INFO] artifact org.apache.maven.plugins:maven-compiler-plugin: \\ checking for updates from central...[INFO] [resources:resources]...[INFO] [compiler:compile]Compiling 1 source file to &lt;dir&gt;/my-app/target/classes[INFO] ----------------------------------------------------------------------------[INFO] BUILD SUCCESSFUL[INFO] ----------------------------------------------------------------------------[INFO] Total time: 3 minutes 54 seconds[INFO] Finished at: Fri Sep 23 15:48:34 GMT-05:00 2005[INFO] Final Memory: 2M/6M[INFO] ---------------------------------------------------------------------------- 第一次你执行这个命令，Maven将会下载执行此命令的所有的插件和相关的依赖。初始安装Maven，这个过程将会需要一段时间来完成(从上面的输出来看，这将会需要大约4分钟)。如果你再次执行这个命令，Maven会拥有它所有的需要的，因此它不会下载任何新的东西，并且会很快的执行这条命令。 正如你从输出中可以看到的，编译后的class文件将会被放在${basedir}/target/classes,这是Maven的另外一个通常的约定。因此，如果你的观察很敏锐，你会发现利用通常的约定，POM篇幅很小，你不用很明确的告诉Maven你的源码在哪里，应该向什么地方输出。通过利用通常的Maven约定，你花费很小的力气可以获得很多的便利！ 我如何编译测试源码并且做单元测试？现在你成功的编译了你的应用源码并且现在你需要做一些单元测试，这些测试代码需要编译和执行(因为每个程序员总是编写和执行他们的单元测试)执行 如下的命令：1mvn test 执行这条命令，你应该看到如下的输出：123456789101112131415161718192021222324252627282930313233[INFO] ----------------------------------------------------------------------------[INFO] Building Maven Quick Start Archetype[INFO] task-segment: [test][INFO] ----------------------------------------------------------------------------[INFO] artifact org.apache.maven.plugins:maven-surefire-plugin: \\ checking for updates from central...[INFO] [resources:resources][INFO] [compiler:compile][INFO] Nothing to compile - all classes are up to date[INFO] [resources:testResources][INFO] [compiler:testCompile]Compiling 1 source file to C:\\Test\\Maven2\\test\\my-app\\target\\test-classes...[INFO] [surefire:test][INFO] Setting reports dir: C:\\Test\\Maven2\\test\\my-app\\target/surefire-reports ------------------------------------------------------- T E S T S-------------------------------------------------------[surefire] Running com.mycompany.app.AppTest[surefire] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 0 sec Results :[surefire] Tests run: 1, Failures: 0, Errors: 0 [INFO] ----------------------------------------------------------------------------[INFO] BUILD SUCCESSFUL[INFO] ----------------------------------------------------------------------------[INFO] Total time: 15 seconds[INFO] Finished at: Thu Oct 06 08:12:17 MDT 2005[INFO] Final Memory: 2M/8M[INFO] ---------------------------------------------------------------------------- 一些需要注意的是：-Maven这次下载了更多的依赖，这些事执行测试需要的依赖和插件，但是对于已经存在的是不会下载的(本地仓库)-在编译和执行测试之前，Maven编译主要的代码(所有的class文件都是最新的，只要我们在编译以后不改变任何东西)如果你简单的想编译你的测试代码(并不去执行)，你可以执行如下命令：1mvn test-compile 现在你可以编译你的应用源码，编译你的测试代码，并且执行测试，你想继续下一步，你将会问… 我如何创建一个jar文件并且将之安装到我的本地仓库？创建一个jar文件很简单，执行如下命令就可以了：1mvn package 如果你看一下你项目的POM你就会发现packaging元素是被设置为jar的。这就是为什么Maven知道去生成一个jar文件的原因了。你查看一下${basedir}/target目录你就会看到生成的jar文件。现在你将想去安装你生成的artifact(jar文件)到你的本地仓库(${user.home/.m2/repository是默认位置)。想了解更到关于仓库的信息，你可以查看仓库介绍，接着让我们继续安装我们的artifact执行如下的命令：1mvn install 执行命令后你应该看到如下的输出：12345678910111213141516171819202122232425262728293031323334[INFO] ----------------------------------------------------------------------------[INFO] Building Maven Quick Start Archetype[INFO] task-segment: [install][INFO] ----------------------------------------------------------------------------[INFO] [resources:resources][INFO] [compiler:compile]Compiling 1 source file to &lt;dir&gt;/my-app/target/classes[INFO] [resources:testResources][INFO] [compiler:testCompile]Compiling 1 source file to &lt;dir&gt;/my-app/target/test-classes[INFO] [surefire:test][INFO] Setting reports dir: &lt;dir&gt;/my-app/target/surefire-reports ------------------------------------------------------- T E S T S-------------------------------------------------------[surefire] Running com.mycompany.app.AppTest[surefire] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 0.001 sec Results :[surefire] Tests run: 1, Failures: 0, Errors: 0 [INFO] [jar:jar][INFO] Building jar: &lt;dir&gt;/my-app/target/my-app-1.0-SNAPSHOT.jar[INFO] [install:install][INFO] Installing &lt;dir&gt;/my-app/target/my-app-1.0-SNAPSHOT.jar to \\ &lt;local-repository&gt;/com/mycompany/app/my-app/1.0-SNAPSHOT/my-app-1.0-SNAPSHOT.jar[INFO] ----------------------------------------------------------------------------[INFO] BUILD SUCCESSFUL[INFO] ----------------------------------------------------------------------------[INFO] Total time: 5 seconds[INFO] Finished at: Tue Oct 04 13:20:32 GMT-05:00 2005[INFO] Final Memory: 3M/8M[INFO] ---------------------------------------------------------------------------- 注意surefire插件(用于执行测试)通过特定的文件命名约定去寻找测试代码。默认的命名规则如下：**/*Test.java**/Test*.java**/*TestCase.java但是默认的不包括如下：**/Abstract*Test.java**/Abstract*TestCase.java你已经走了初始化、构建、测试、打包和安装整个流程，这是Maven项目的典型过程。如果你已经注意的话，这就是大部分项目利用Maven来做的工作。所有你能够做的都在这个18行的文件中即项目模型或POM。如果你了解利用Ant创建相同功能的文件，你会发现篇幅一般是POM的两倍。Maven可以给你提供更多的功能而且不用更多的设置。利用Ant得到更多的功能，你必须注意各种容易错的条件。什么东西还是你可以免费得到的呢？这里有很多的Maven插件。我们这里提及一个插件，它是Maven的一个重要特性之一。POM中不用任何多余的设置，可以为你的项目生成一个站点。你可能会想去定制化你的Maven站点，但是假如你想很快为你的项目提供一个基本信息的介绍，那么可以执行如下的命令：1mvn site 这里也有很多独立的功能可以去执行，例如：1mvn clean 这个命令将会删除target目录和其中的所有构建数据，就类似开始的样子.可能你想为项目生成一个intellij idea的描述？1mvn idea:idea 这样就可以运行在IDEA中，这将会更新设置，而不是恢复成最开始的样子。如果你使用Eclipse IDE,只需：1mvn eclipse:eclipse 注意：许多类似的Maven1.0目标还存在，比如jar:jar,但是他们的结果可能和你所期望的是不一样的，现在jar:jar不会重新编译源码，它只会简单的利用target/classes来创建jar文件，会假设所有的东西在之前已经做了，比如说编译。 什么是SNAPSHOT版本？注意pom.xml中版本号的值会带有-SNAPSHOT的后缀。12345678&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; ... &lt;groupId&gt;...&lt;/groupId&gt; &lt;artifactId&gt;my-app&lt;/artifactId&gt; ... &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;name&gt;Maven Quick Start Archetype&lt;/name&gt; ... SNAPSHOT指开发分支的最新的代码，无法保证代码是稳定的或者不会改变的。相反的，release版本的代码是不会改变的。换句话说，SNAPSHOT版本是release之前的开发版本。在发布过程中，版本例如x.y-SNAPSHOT会变成x.y。发布过程经常会增量开发版本为x.(y+1)-SNAPSHOT。比如：版本号1.0-SNAPSHOT的发布版本为1.0,并且新的开发版本为1.1-SNAPSHOT。 我如何使用插件？无论何时你想去定制化你的Maven项目，所需做的就是添加或重新配置插件。Maven 1.0的用户请注意：在Maven1.0中，你可能会添加一些preGoal到maven.xml并且在project.properties中添加一些条目。这里有些变化。 比如，我们将会配置java编译器能够编译jdk 5.0的代码，只需在你的POM中如下配置即可：12345678910111213&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.3&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.5&lt;/source&gt; &lt;target&gt;1.5&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 你会注意所有的Maven插件都在某种程度上类似依赖，插件会自动的下载和使用，而且是一个确定的版本。configuration元素会将给定的参数应用到每个目标。在以上的案例中，编译插件被作为构建过程的一部分，也可以在过程中添加新的目标并配置指定的目标。想了解更多的信息，请看生命周期构建简介想查看什么配置适合一个插件，你可以看插件列表并导航到你在使用的插件和目标。对于怎样配置插件的合适参数，可以查看插件配置指南 我如何将资源添加到jar文件当中？另外一个很常用的案例是不需要做任何POM改变，可以将资源我文件打包到Jar文件当中。对于这个功能，Maven需要依赖标准的目录结构,意思是利用标准的Maven约定将资源放在约定的目录结构，你可以将资源打包到jar文件。看下边的例子，我们可以将想打包的资源文件放在目录${basedir}/src/main/resources。这个简单的规则是说:任何在{basedir}/src/main/resources目录下的目录或文件都会打包到你的jar文件中，而且目录结构也是一样的，基目录的jar文件的根目录。123456789101112131415161718my-app|-- pom.xml`-- src |-- main | |-- java | | `-- com | | `-- mycompany | | `-- app | | `-- App.java | `-- resources | `-- META-INF | `-- application.properties `-- test `-- java `-- com `-- mycompany `-- app `-- AppTest.java 在打包的jar文件中会有一个META-INF目录中有文件appllication.properties。如果你解包jar文件，你可以看到类似如下：123456789101112|-- META-INF| |-- MANIFEST.MF| |-- application.properties| `-- maven| `-- com.mycompany.app| `-- my-app| |-- pom.properties| `-- pom.xml`-- com `-- mycompany `-- app `-- App.class 你可以看到${basedir}/src/main/resources可以在jar文件的根目录下找到，并且在META-INF目录下可以找到application.properties。你同样会注意到一些其他的文件比如META-INF/MANIFEST.MF，pom.xml和pom.properties，这些事Maven默认生成的文件。你可以创建你自己的manifest,如果你不创建，Maven会默认创建一个(你同样可以改变默认的manifest，我们将会在一会说到这个)。pom.xml和pom.properties文件被打包到jar中，因此每个artifact都是独立的，如果你需要的话，你可以利用你自己应用的元数据。一个简单的应用就是可以恢复你应用的版本号。配置文件POM需要你利用一些Maven工具，但是properties可以利用java api，比如：12345#Generated by Maven#Tue Oct 04 15:43:21 GMT-05:00 2005version=1.0-SNAPSHOTgroupId=com.mycompany.appartifactId=my-app 为将资源文件为你的单元测试加载到类路径下，你需要遵循与你添加资源文件到jar中同样的模式，这是你应该拥有一个项目目录结构类似下边：1234567891011121314151617181920my-app|-- pom.xml`-- src |-- main | |-- java | | `-- com | | `-- mycompany | | `-- app | | `-- App.java | `-- resources | `-- META-INF | |-- application.properties `-- test |-- java | `-- com | `-- mycompany | `-- app | `-- AppTest.java `-- resources `-- test.properties 在一个单元测试中你可以利用一小段代码如下去访问需要的资源文件：12345...// Retrieve resourceInputStream is = getClass().getResourceAsStream( &quot;/test.properties&quot; );// Do something with the resource... 我如何过滤资源文件？有时候资源文件需要包含一个值，这个值只能在构建时期使用。为了完成这个功能，利用${&lt;propertyname&gt;}就可以将这个值带入你的资源文件。这个属性可以是你在pom.xml中定义的一个值，用户的setting.xml中的一个值，外部properties文件中的一个值，或者是一个系统属性。为了在复制时利用Maven过滤资源，简单的设置filtering为真即可：1234567891011121314151617181920212223242526272829303132&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.mycompany.app&lt;/groupId&gt; &lt;artifactId&gt;my-app&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;Maven Quick Start Archetype&lt;/name&gt; &lt;url&gt;http://maven.apache.org&lt;/url&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.11&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;resources&gt; &lt;resource&gt; &lt;directory&gt;src/main/resources&lt;/directory&gt; &lt;filtering&gt;true&lt;/filtering&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;/build&gt;&lt;/project&gt; 你会注意到我们不得不增加build,resources,resource元素，此外我们不得不明确声明资源文件的位置是src/mai/resources目录。左右的这些信息都是默认提供的，但是因为默认的filtering是false的，我们不得不增加这些信息到我们的pom.xml,为了去重写默认值并且将filtering设置为true。为了引用你在pom.xml中定义的属性，属性名称利用xml元素名称去定义值，pom运行将项目元素作为一个别名，因此${project.version}指的是项目的名称，${project.version}指的是你项目的版本，${project.build.finalName}指的是在项目打包时生成的文件的最终的名称等等。注意一些POM元素拥有默认值，因此不需要明确的在pom.xml中定义。同样，用户的setting.xml中的值可以利用一settings开头的属性名来引用(比如,${settings.localRepository}指的是用户的本地路径)。继续我们的例子，让我们在application.properties中增加两个属性(这个文件我们放到src/main/resources).当资源文件被过滤后，这个文件中的值将会被应用(过滤可以看作扫描)：123# application.propertiesapplication.name=$&#123;project.name&#125;application.version=$&#123;project.version&#125; 到这里，你可以执行如下的命令(process-resources是构建生命周期阶段资源的复制和筛选)：1mvn process-resources application.properties文件在target/classes就像如下：123# application.propertiesapplication.name=Maven Quick Start Archetypeapplication.version=1.0-SNAPSHOT 为了引用定义在外部文件中的属性，你所做的事需要增加一个外部文件的引用到你的pom.xml文件中。首先，让我们创建外部属性文件src/main/filters/filter.properties:12# filter.propertiesmy.filter.value=hello! 接着我们将在pom.xml中引用这个文件：1234567891011121314151617181920212223242526272829303132333435&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.mycompany.app&lt;/groupId&gt; &lt;artifactId&gt;my-app&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;Maven Quick Start Archetype&lt;/name&gt; &lt;url&gt;http://maven.apache.org&lt;/url&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.11&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;filters&gt; &lt;filter&gt;src/main/filters/filter.properties&lt;/filter&gt; &lt;/filters&gt; &lt;resources&gt; &lt;resource&gt; &lt;directory&gt;src/main/resources&lt;/directory&gt; &lt;filtering&gt;true&lt;/filtering&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;/build&gt;&lt;/project&gt; 接着，如我我们可以在application.properties中引用这个属性：1234# application.propertiesapplication.name=$&#123;project.name&#125;application.version=$&#123;project.version&#125;message=$&#123;my.filter.value&#125; 下一步执行mvn process-resources命令将会把我们的新属性值放进application.properties。除了定义my.filter.value在外部文件，你也可以定义其在pom文件中的properties小结，你可以达到相同的效果：123456789101112131415161718192021222324252627282930313233343536&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.mycompany.app&lt;/groupId&gt; &lt;artifactId&gt;my-app&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;Maven Quick Start Archetype&lt;/name&gt; &lt;url&gt;http://maven.apache.org&lt;/url&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.11&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;resources&gt; &lt;resource&gt; &lt;directory&gt;src/main/resources&lt;/directory&gt; &lt;filtering&gt;true&lt;/filtering&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;/build&gt; &lt;properties&gt; &lt;my.filter.value&gt;hello&lt;/my.filter.value&gt; &lt;/properties&gt;&lt;/project&gt; 过滤资源同样可以获取系统属性；java内建的系统属性(比如java.version或者user.home)或者利用java -D 参数定义在命令行的属性。继续我们的例子，让我们改变application.properties文件像如下：123# application.propertiesjava.version=$&#123;java.version&#125;command.line.prop=$&#123;command.line.prop&#125; 现在，当我们执行如下的命令(注意定义在命令行的command.line.prop的属性)，application.properties文件将会包含属性系统属性的值.1mvn process-resources &quot;-Dcommand.line.prop=hello again&quot; 我如何使用外部的依赖？在之前的例子中，你可能已经注意有一个dependencies元素。实际上你一直在使用一个外部的依赖，但是现在我们讨论一些其工作的细节。想深入了解，请查看依赖机制介绍dependencies小结列出了所有在项目构建过程(无论是变异期、测试期、运行期或者其他)中需要的外部依赖。现在我们的项目只依赖JUnit(为了清晰，我将所有的关于资源的东西都提取出来):1234567891011121314151617181920212223&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.mycompany.app&lt;/groupId&gt; &lt;artifactId&gt;my-app&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;Maven Quick Start Archetype&lt;/name&gt; &lt;url&gt;http://maven.apache.org&lt;/url&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.11&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 对于每个外部一拉，你需要定义至少四个东西：groupId,artifactId,version和scope,groupId,artifactId和version和上边介绍的pom.xml是一样的。scope元素表明你的项目怎样使用此依赖，值可以是compile,test和runtime。要了解更多的信息，你可以指定一个依赖，并查看项目描述符参考根据这个依赖信息，Maven能够在构建项目是引用依赖Maven从哪里引用依赖呢？Maven会从你的本地仓库中寻找所有的依赖。在之前的章节，我们将artifact(my-app-1.0-SNAPSHOT.jar)安装到了本地仓库.一旦被安装到了本地仓库，另外一个项目也可以引用这个jar文件作为依赖，只需像之前那样讲依赖信息添加到pom.xml文件中就可以了:1234567891011121314151617&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;groupId&gt;com.mycompany.app&lt;/groupId&gt; &lt;artifactId&gt;my-other-app&lt;/artifactId&gt; ... &lt;dependencies&gt; ... &lt;dependency&gt; &lt;groupId&gt;com.mycompany.app&lt;/groupId&gt; &lt;artifactId&gt;my-app&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 假如依赖在其他地方使用呢？他们怎样来访问我的本地仓库呢？当一个项目引用依赖在本地仓库不可得是，Maven将会从远程仓库下载依赖到本地仓库。你可能注意到maven下载了许多东西在你勾线你的第一个项目的时候(折现下载的依赖是一些用于构建项目的插件)。默认远程仓库是http://repo.maven.apache.org/maven2/。你同样可以建立自己的远程仓库(可能是你公司的中央仓库)用于替换默认的远程仓库，或者增加一个。了解更多关于仓库的信息你可以引用仓库介绍。让我们添加另外一个依赖到我们的项目。现在我们增加以下日志到代码并且需要增加log4j作为一个依赖。首先我们需要知道log4j的groupId,artifactId以及version等信息。我们可以浏览ibiblio找到这些，或者利用google搜索site:www.ibiblio.org maven2 log4j。搜索会给你展示一个目录/maven2/log4j/log4j或者/pub/packages/maven2/log4j/log4j。在这个目录下有一个文件叫作maven-metadata.xml。这里是log4j的maven-meatdata.xml：123456789101112131415161718&lt;metadata&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.1.3&lt;/version&gt; &lt;versioning&gt; &lt;versions&gt; &lt;version&gt;1.1.3&lt;/version&gt; &lt;version&gt;1.2.4&lt;/version&gt; &lt;version&gt;1.2.5&lt;/version&gt; &lt;version&gt;1.2.6&lt;/version&gt; &lt;version&gt;1.2.7&lt;/version&gt; &lt;version&gt;1.2.8&lt;/version&gt; &lt;version&gt;1.2.11&lt;/version&gt; &lt;version&gt;1.2.9&lt;/version&gt; &lt;version&gt;1.2.12&lt;/version&gt; &lt;/versions&gt; &lt;/versioning&gt;&lt;/metadata&gt; 从这个文件里，我们可以看到groupId为log4j并且artifactId为log4j。我们有不多的版本可以进行选择；现在我们用罪行的版本,1.2.12(一些maven-metadata.xml文件可能指定哪个版本是最新的发布版本)。在目录下我们还可以看到每个版本的log4j库。在里边，我们可以看到jar文件(比如log4j-1.2.12.jar)和一个pom文件(这个是依赖的pom文件，表明它拥有的更多的依赖和其他信息)以及另外一个maven-metadata.xml文件。这里还有一个md5文件包含这些文件的MD5哈希值。你可以用这个去验证一个库或者知道你现在正在使用的库的版本号。现在我们知道我们需要的信息，我们可以吧依赖添加到我们的pom.xml文件中。1234567891011121314151617181920212223242526272829&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.mycompany.app&lt;/groupId&gt; &lt;artifactId&gt;my-app&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;Maven Quick Start Archetype&lt;/name&gt; &lt;url&gt;http://maven.apache.org&lt;/url&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.11&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.12&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 我如何发布jar到远程仓库？对于将jar文件发布到外部仓库，你应该在pom.xml中配置url并且在settings.xml中配置连接到仓库的验证信息。这里是个例子，用scp和用户名/密码验证：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.mycompany.app&lt;/groupId&gt; &lt;artifactId&gt;my-app&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;Maven Quick Start Archetype&lt;/name&gt; &lt;url&gt;http://maven.apache.org&lt;/url&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.11&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.codehaus.plexus&lt;/groupId&gt; &lt;artifactId&gt;plexus-utils&lt;/artifactId&gt; &lt;version&gt;1.0.4&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;filters&gt; &lt;filter&gt;src/main/filters/filters.properties&lt;/filter&gt; &lt;/filters&gt; &lt;resources&gt; &lt;resource&gt; &lt;directory&gt;src/main/resources&lt;/directory&gt; &lt;filtering&gt;true&lt;/filtering&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;/build&gt; &lt;distributionManagement&gt; &lt;repository&gt; &lt;id&gt;mycompany-repository&lt;/id&gt; &lt;name&gt;MyCompany Repository&lt;/name&gt; &lt;url&gt;scp://repository.mycompany.com/repository/maven2&lt;/url&gt; &lt;/repository&gt; &lt;/distributionManagement&gt;&lt;/project&gt; 12345678910111213141516&lt;settings xmlns=&quot;http://maven.apache.org/SETTINGS/1.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd&quot;&gt; ... &lt;servers&gt; &lt;server&gt; &lt;id&gt;mycompany-repository&lt;/id&gt; &lt;username&gt;jvanzyl&lt;/username&gt; &lt;!-- Default value is ~/.ssh/id_dsa --&gt; &lt;privateKey&gt;/path/to/identity&lt;/privateKey&gt; (default is ~/.ssh/id_dsa) &lt;passphrase&gt;my_key_passphrase&lt;/passphrase&gt; &lt;/server&gt; &lt;/servers&gt; ...&lt;/settings&gt; 现在如果你连接到了匿名登录的openssh ssh服务器，你每次都需要输入你的用户名和密码去验证(尽管你可以利用另外一个ssh客户端去登录)。这种情况你可以利用公钥去登录验证。假如在settings.xml中有密码这种情况应该注意，了解更多信息，请看密码加密 我如何创建帮助文档？在创建你的帮助文档之前，你可以利用archetype机制来为你的项目生成一个网站，利用如下命令：12345mvn archetype:generate \\ -DarchetypeGroupId=org.apache.maven.archetypes \\ -DarchetypeArtifactId=maven-archetype-site \\ -DgroupId=com.mycompany.app \\ -DartifactId=my-app-site 现在你可以到创建站点指南去学习如何为你的项目创建帮助文档。 我如何构建其他类型的项目？注意生命周期适用于任何项目类型。比如，回到基本目录我们可以创建一个简单的web应用：12345mvn archetype:generate \\ -DarchetypeGroupId=org.apache.maven.archetypes \\ -DarchetypeArtifactId=maven-archetype-webapp \\ -DgroupId=com.mycompany.app \\ -DartifactId=my-webapp 注意这些都必须位于同一行。这将会创建一个目录叫作my-webapp包含如下的项目描述：123456789101112131415161718192021222324&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.mycompany.app&lt;/groupId&gt; &lt;artifactId&gt;my-webapp&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;war&lt;/packaging&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.11&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;finalName&gt;my-webapp&lt;/finalName&gt; &lt;/build&gt;&lt;/project&gt; 注意&lt;packaging&gt;元素,这个告诉maven打包类型为war，进入webapp的项目目录并且尝试：1mvn clean package 你将会看到target/my-webapp.war被创建，到目前为止，所有的正常步骤都被执行。 我如何同时构建多个项目？这个概念是说利用Maven构建多个模块。在这节，我们将会展示如何同时构建上述的war包括先前的jar。首先我们需要一个父pom.xml文件在另外两个的上层目录，例如：1234567891011+- pom.xml+- my-app| +- pom.xml| +- src| +- main| +- java+- my-webapp| +- pom.xml| +- src| +- main| +- webapp 这个pom文件你将会创建包含如下的：12345678910111213141516&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.mycompany.app&lt;/groupId&gt; &lt;artifactId&gt;app&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;pom&lt;/packaging&gt; &lt;modules&gt; &lt;module&gt;my-app&lt;/module&gt; &lt;module&gt;my-webapp&lt;/module&gt; &lt;/modules&gt;&lt;/project&gt; jar需要依赖webapp，因此我们向my-webapp/pom.xml添加如下：123456789... &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.mycompany.app&lt;/groupId&gt; &lt;artifactId&gt;my-app&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; ... &lt;/dependencies&gt; 接着向另外子目录下的pom.xml文件中添加&lt;parent&gt;标签：12345678910&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;parent&gt; &lt;groupId&gt;com.mycompany.app&lt;/groupId&gt; &lt;artifactId&gt;app&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/parent&gt; ... 现在在顶层目录尝试运行：1mvn clean install war文件将会被创建在my-webapp/target/my-webapp.war中，同样jar也会被创建：12345678910111213$ jar tvf my-webapp/target/my-webapp-1.0-SNAPSHOT.war 0 Fri Jun 24 10:59:56 EST 2005 META-INF/ 222 Fri Jun 24 10:59:54 EST 2005 META-INF/MANIFEST.MF 0 Fri Jun 24 10:59:56 EST 2005 META-INF/maven/ 0 Fri Jun 24 10:59:56 EST 2005 META-INF/maven/com.mycompany.app/ 0 Fri Jun 24 10:59:56 EST 2005 META-INF/maven/com.mycompany.app/my-webapp/3239 Fri Jun 24 10:59:56 EST 2005 META-INF/maven/com.mycompany.app/my-webapp/pom.xml 0 Fri Jun 24 10:59:56 EST 2005 WEB-INF/ 215 Fri Jun 24 10:59:56 EST 2005 WEB-INF/web.xml 123 Fri Jun 24 10:59:56 EST 2005 META-INF/maven/com.mycompany.app/my-webapp/pom.properties 52 Fri Jun 24 10:59:56 EST 2005 index.jsp 0 Fri Jun 24 10:59:56 EST 2005 WEB-INF/lib/2713 Fri Jun 24 10:59:56 EST 2005 WEB-INF/lib/my-app-1.0-SNAPSHOT.jar 这个是怎么创建的？首先父pom创建(称作app),有一个packaging和一系列modules定义。这个告诉Maven在项目集合上去运行操作，而不是当前这个(重写这个行为，你可以利用--non-recursive命令)接着，我们告诉war需要my-app的jar.这个做了一些事情：可以是其在类路径上，war中的其他代码可以找到，保证jar的构建总是先于war的，并且表明war插件将jar包含到它的库目录下。你可能注意到junit-4.11.jar是一个依赖，但是并没有在war中结束(与生命周期有关，因此用结束来修饰)。原因是&lt;scope&gt;test&lt;/scope&gt;原色，这个仅用来测试，并且不会包含在web应用当中。最后一步是添加parent定义，这个不同于extend元素(maven 1.0):这个保证POM总是可以定位到，即使项目与其双亲是分布式的，以查找仓库的方式。不想Maven1.0，不需要你运行install去成功执行这些步骤，你可以运行package，可以在target目录中生成，而不会是本地仓库。你可能喜欢再次生成你的IDEA工作区在顶层目录…1mvn idea:idea","categories":[{"name":"Java","slug":"Java","permalink":"peachey.blog/categories/Java/"}],"tags":[{"name":"Maven","slug":"Maven","permalink":"peachey.blog/tags/Maven/"}]},{"title":"深入理解java泛型","slug":"java-generic","date":"2016-07-22T15:27:45.000Z","updated":"2018-07-22T19:51:23.678Z","comments":true,"path":"2016/07/22/java-generic/","link":"","permalink":"peachey.blog/2016/07/22/java-generic/","excerpt":"","text":"目录 何为泛型 为何要引入泛型，即泛型与Object的优势 类型擦除和原始类型 泛型限定 泛型的一些基本规则约束 何为泛型首先泛型的本质便是类型参数化，通俗的说就是用一个变量来表示类型，这个类型可以是String, Integer等等不确定，表明可接受的类型，原理类似如下代码： 123int pattern;pattern = 4;pattern = 5; 泛型的具体形式见泛型类、泛型方法, 泛型的具体类型如下：1234567891011121314class Test&lt;T&gt; &#123; private T t; Test(T t)&#123; this.t = t; &#125; public T getT()&#123; return t; &#125; public void setT(T t)&#123; this.t = T; &#125;&#125; 泛型方法举例：123public &lt;T&gt; void show()&#123; ...&#125; 泛型参数类型必须在返回类型之前 为何要引入泛型，即泛型与Object的优势由于泛型可以接受多个参数，而Object经过强制类型转换可以转换为任何类型，既然二者都具有相同的作用，为何还要引进泛型呢？因为泛型可以把使用Object的错误提前到编译后，而不是运行后，提升安全性。以下用带泛型的ArrayList和不带泛型的ArrayList举例说明： 12345ArrayList al = new ArrayList();al.add(\"hello\");al.add(4); //自动装箱String s1 = (String)al.get(0);String s2 = (String)al.get(1); //在编译时没问题，但在运行时出现问题 首先声明无泛型的ArrayList时，其默认的原始类型是Object数组，既然为Object类型，就可以接受任意数据的赋值，因此编译时没有问题，但是在运行时，Integer强转成String, 肯定会出现ClassCastException,因此泛型的引入增强了安全性，把类转换异常提前到了编译时期。 类型擦除和原始类型类型擦除的由来由于Java的虚拟机中并不存在泛型，泛型只是为了完善java体系，增加程序员的编辑性以及安全性而创建的一种机制，在Java虚拟机对应泛型的都是确定的类型，在编写泛型代码后，java虚拟中会吧会这些泛型参数类型都擦除，用响应的确定类型来代替，代替的这一动作叫做类型擦除，而用于替代的类型称为原始类型，在类型擦除过程中，一般使用第一个限定的类型来替换，若无限定则使用Object。 对泛型类的翻译，泛型类（不带泛型限定）代码：123456class Test&lt;T&gt; &#123; private T t; public void show(T t)&#123; &#125;&#125; 虚拟机进行翻译后的原始类型：123456class Test &#123; private Object t; public void show(Object t)&#123; &#125;&#125; 泛型类（带泛型限定）代码： 123456class Test&lt;? extends Comparable&gt; &#123; private T t; public void show(T t)&#123; &#125;&#125; 虚拟机进行翻译后的原始类型： 123456class Test&#123; private Comparable t; public void show(Comparable t)&#123; &#125;&#125; 泛型方法的翻译 12345678910111213class Test&lt;T&gt;&#123; private T t; public void show(T t)&#123; &#125;&#125;class TestDemo extends Test&lt;String&gt;&#123; private String t; public void show(String t)&#123; &#125;&#125; 由于TestDemo继承Test&lt;String&gt;, 但是Test在类型擦除后还有一个public void show(Object t), 这和那个show(String s)出现重载，但是本意却是没有show(Ojbect t)的，因此在虚拟机翻译泛型方法中，引入了桥方法，及在类型擦除后的show(Object t)中调用另一个方法，代码如下： 123public void show(Object t)&#123;&#125; 泛型限定泛型限定是通过?来实现的，表示可以接受任意类型，那一定有人有疑问，那?和T（二者单独使用时）有啥区别了，其实区别也不是很大，紧急在对参数类型的使用上。 1234567891011121314public void print(ArrayList&lt;?&gt; al)&#123; Iterator&lt;?&gt; it = al.iterator(); while(it.hasNext())&#123; System.our.println(it.next()); &#125;&#125;public &lt;T&gt; void print(ArrayList&lt;T&gt; al)&#123; Iterator&lt;T&gt; it = al.iterator(); while(it.hasNext())&#123; T t = it.next(); // 区别就在此处，T可以作为类型来使用，而?仅能作为接收任意类型 System.out.println(t); &#125;&#125; ? extends SomeClass: 这种限定，说明的是只能接收SomeClass及其子类类型，所谓的”上限”。? extends SomeClass: 这种限定，说明只能接收SomeClass及其父类类型，所谓的”下限”。 以下举例? extends SomeClass说明以下这类限定的一种应用方式，由于泛型参数类型可以表示任意类型和类型，若T要引用compareTo方法，如何保证在T类型中定义了compareTo方法呢？利用如下代码： 123public &lt;T extends Comparable&gt; show(T a, T b)&#123; int num = a.compareTo(b);&#125; 此处用于限定T类型继承自Comparable,因为T类型可以调用compareTo方法。 还可以有多个类型限定，例如： 1&lt;T extends Comparable &amp; Serializable&gt; 这种书写方式为何把Comparable写在前边？因为由于类型擦除的问题，原始类型是由Comparable替换的，因此写在前边的是类中存在此类型的泛型方法放在前边，避免调用方法时类型的强制转换，提高效率。 123456789class Test&lt;T extends Comparable &amp; Serializable&gt;&#123; private T lower; private T upper; public Test(T first, T second)&#123; //此处是利用Comparable的方法，因此把Comparable写在前边，类型擦除后为Comparable，若为Serializable,还得用强制类型转换，否则不能使用compareTo方法。 int a = first.compareTo(second); ... &#125;&#125; 关于泛型类型限定的”继承”误区，总有些人误把类型的限定当做继承，比如： 1234//类型是这样的&lt;Student extends Person&gt;//然后出现此类错误ArrayList&lt;Person&gt; al = new ArrayList&lt;Student&gt;(); 此处的&lt;Person&gt;, &lt;Student&gt;作为泛型的意思是ArrayList容器的接受类型，用一个简单的例子来说明：ArrayList是一个大型的养殖场，&lt;Person&gt;说明的是他能够接收动物，而上边的new语句生成的是一个只能够接收猪的养殖场（ArrayList&lt;Student&gt;）, 即把一个大型养殖场建造成了一个养猪场，若是继承的大型养殖场肯定是还能接受狗、羊…的，但是现在建造成了养猪场，那还能接受别的动物么？所以还肯定是错误的用法！简而言之，泛型new时两边的类型参数必须一致。 泛型的一些基本规则约束 泛型的类型参数必须为类的引用，不能用基本类型(int,short,long,byte,float,double,char,boolean) 泛型的类型的参数化，在使用时可以用作不同类型（此处在说泛型类时会详细说明） 泛型的类型参数可以有多个，代码举例如下： 123public &lt;T, E&gt; void show()&#123; coding operation..... &#125; 泛型可以使用extends, super, ?来对类型参数进行限定。 由于类型擦除，运行时类型查询只使用于原始类型，比如instanceof，getClass()、强制类型转换，a instanceof (Pair&lt;Employee&gt;), 在类型擦除后便是a instanceof Pair, 因此以上运行的一些操作在虚拟机中操作都是对原始类型进行操作，无论写的多么魔幻，都逃不出类型擦除，因为在虚拟机种并不存在泛型。 不能创建参数化类型的数组 例如写如下代码： 1234567Pair&lt;String&gt;[] table = new Pair&lt;String&gt;[10]; //ERROR//让Object[] t指向tableObject[] t = table;//向t中添加对象t[0] = new Pair&lt;Employe&gt;();//关键错误之处String s = table[0]; 由于Object可以接收任何类型，在里边存入new Pair&lt;Employee&gt;时，没有任何问题，但是当取出的时候会出现ClassCastException,因此不能创建参数化类型数组。 不能实例化类型变量，及不能出现以下的类似代码 123T t = new T();//或T.Class 因为在类型擦除后，便是Object t = new Object();与用意不符合，即本意肯定不是要调用Object. 不能再静态域或方法中出现参数类型,例如如下错误代码 123456789101112class Test&lt;T&gt;&#123; private static T example; //error public static void showExample() //error &#123; action about T... &#125; public static T showExample() //error &#123; action about T.... &#125;&#125; 首先方法是一个返回类型为T的普通方法，而非泛型方法，这和在静态方法中使用非静态参数是一样的，静态方法是先于对象而存在内存中的，因此在编译的时候，T的类型无法确定，一定会出现“Cannot make a static reference to a non_static reference”这样类似的错误。 但是这样的代码就是正确的 123456class Test&lt;T&gt;&#123; public static &lt;T&gt; T show() &#123; action &#125;&#125; 因为此处的静态方法是泛型方法，可以使用. 不能抛出或捕获泛型类的实例不能抛出不能捕获泛型类对象泛型类不能扩展Throwable，注意是类不能继承Throwable，类型参数的限定还是可以的。catch子句不能使用类型变量，如下代码: 12345678try&#123; ....&#125;catch(T e) //error&#123; ...&#125; 类型擦除后的冲突注意, 例如 123456class Pair&lt;T&gt;&#123; public boolean equals(T value) //error &#123; .... &#125;&#125; 此处的错误的原因不能存在同一个方法，在类型擦除后，Pair的方法为，public boolean equals(Object value),这与从Object.class中继承下来的equals(Object obj)冲突。 一个类不能成为两个接口类型的子类，而这两个接口是同一接口的不同参数化。 12class Calendar implements coparable&lt;Calendar&gt;&#123;&#125;class GregorianCalendar extends Calendar implements Comparable&lt;GregorianCalendar&gt;&#123;&#125; //error 当类型擦除后，Calendar实现的是Comparable,而GregorianCalendar继承了Calendar,又去实现Comparable，必然出错！","categories":[{"name":"Java","slug":"Java","permalink":"peachey.blog/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"peachey.blog/tags/Java/"}]}]}